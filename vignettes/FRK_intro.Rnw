\documentclass{article}

% \VignetteEngine{knitr::knitr}
% \VignetteIndexEntry{Spatial and spatio-temporal kriging with FRK}

\usepackage{hyperref}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{array}
\usepackage[normalem]{ulem}
\usepackage{amsthm}
\usepackage{txfonts}

\DeclareSymbolFont{matha}{OML}{txmi}{m}{it}% txfonts
\DeclareMathSymbol{\varv}{\mathord}{matha}{118}

\renewcommand{\tt} {\texttt}

\doublespacing

\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]

\newcommand{\red}{\textcolor{red}}%
\newcommand{\blue}{\textcolor{blue}}

\newcommand{\zerob} {{\bf 0}}
\newcommand{\oneb} {{\bf 1}}
\newcommand{\expect} {{\mathbb{E}}}
\newcommand{\pt} {\tilde{p}}
\newcommand{\alphat} {\tilde{\alpha}}
\newcommand{\betat} {\tilde{\beta}}
\newcommand{\pb} {\bar{p}}
\newcommand{\thetab} {{\boldsymbol{\theta}}}
\newcommand{\alphab} {{\boldsymbol{\alpha}}}
\newcommand{\kappab} {{\boldsymbol{\kappa}}}
\newcommand{\sigmab} {{\boldsymbol{\sigma}}}
\newcommand{\nub} {{\boldsymbol{\nub}}}
\newcommand{\gammab} {{\boldsymbol{\gamma}}}
\newcommand{\deltab} {{\boldsymbol{\delta}}}
\newcommand{\Deltab} {{\boldsymbol{\Delta}}}
\newcommand{\Thetab} {{\boldsymbol{\Theta}}}
\newcommand{\varthetab} {{\boldsymbol{\vartheta}}}
\newcommand{\Yset} {\mathcal{Y}}
\newcommand{\Xset} {\mathcal{X}}
\newcommand{\intd} {\textrm{d}}
\newcommand{\phib} {\boldsymbol{\phi}}
\newcommand{\zetab} {\boldsymbol{\zeta}}
\newcommand{\etab} {\boldsymbol{\eta}}
\newcommand{\psib} {\boldsymbol{\psi}}
\newcommand{\Sigmawinv} {{\boldsymbol{\it \Sigma}}_w^{-1}}
\newcommand{\Sigmamat} {{\bm \Sigma}}
\newcommand{\Sigmamatt} {\widetilde{\boldsymbol{\Sigma}}}
\newcommand{\Qmatt} {\widetilde{\textbf{Q}}}
\newcommand{\muvect} {\widetilde{\boldsymbol{\mu}}}
\newcommand{\Psib} {{\bm \Psi}}
\newcommand{\Omegab} {{\bm \Omega}}
\newcommand{\Upsilonmat} {{\boldsymbol{\it \Upsilon}}}
\newcommand{\Lambdamat} {\mathbf{\Lambda}}
\newcommand{\Gammamat} {{\boldsymbol{\it \Gamma}}}
\renewcommand{\Gammamat} {{\boldsymbol{\Gamma}}}
\newcommand{\Pimat} {{\bm \Pi}}
\newcommand{\Amat} {\textbf{A}}
\newcommand{\Bmat} {\textbf{B}}
\newcommand{\Dmat} {\textbf{D}}
\newcommand{\Dvec} {\textbf{D}}
\newcommand{\Gmat} {\textbf{G}}
\newcommand{\Lmat} {\textbf{L}}
\newcommand{\Qmat} {\textbf{Q}}
\newcommand{\Rmat} {\textbf{R}}
\newcommand{\Smat} {\textbf{S}}
\newcommand{\Tmat} {\textbf{T}}
\newcommand{\Qt} {\widetilde{\textbf{Q}}}
\newcommand{\Qtinv} {\widetilde{\textbf{Q}}^{-1}}
\newcommand{\Mmat} {\textbf{M}}
\newcommand{\Cmat} {\mathbf{C}}
\newcommand{\Jmat} {\mathbf{J}}
\newcommand{\cmat} {\textbf{c}}
\newcommand{\Kmat} {\textbf{K}}
\newcommand{\im} {\iota}
\newcommand{\Zmat} {\textbf{Z}}
\newcommand{\Xmat} {\textbf{X}}
\newcommand{\Xvec} {\mathbf{X}}
\newcommand{\Rvec} {\mathbf{R}}
\newcommand{\Imat} {\textbf{I}}
\newcommand{\Umat} {\textbf{U}}
\newcommand{\Pmat} {\textbf{P}}
\newcommand{\Hmat} {\textbf{H}}
\newcommand{\Vmat} {\textbf{V}}
\newcommand{\bvec} {\textbf{b}}
\newcommand{\dvec} {\textbf{d}}
\newcommand{\avec} {\textbf{a}}
\newcommand{\evec} {\textbf{e}}
\newcommand{\hvec} {\textbf{h}}
\newcommand{\xvec} {\textbf{x}}
\newcommand{\yvec} {\textbf{y}}
\newcommand{\zvec} {\textbf{z}}
\newcommand{\wvec} {\textbf{w}}
\newcommand{\vvec} {\textbf{v}}
\newcommand{\svec} {\textbf{s}}
\newcommand{\tvec} {\textbf{t}}
\newcommand{\uvec} {\textbf{u}}
\newcommand{\gvec} {\textbf{g}}
\newcommand{\fvec} {\textbf{f}}
\newcommand{\rvec} {\textbf{r}}
\newcommand{\muvec} {\boldsymbol{\mu}}
\newcommand{\Psix} {{\boldsymbol{\it \Psi}}_{\xvec}}
\newcommand{\Phimat} {{\boldsymbol{\it \Phi}}}
\newcommand{\Psitheta} {{\boldsymbol{\it \Psi}}_{\varthetab}}
\newcommand{\Psia} {{\boldsymbol{\it \Psi}}_{A}}
\newcommand{\Psixinv} {{\boldsymbol{\it \Psi}}_{\xvec}^{-1}}
\newcommand{\vvm} {\boldsymbol {\mathcal \upsilon}}
\newcommand{\upsilonb} {\boldsymbol {\upsilon}}
\newcommand{\betab} {\boldsymbol {\beta}}
\newcommand{\omegab} {\boldsymbol {\omega}}
\newcommand{\Aop}{\boldsymbol{\mathcal{A}}}
\newcommand{\ICE} {\textit{ICE}}
\newcommand{\GIA} {\textit{GIA}}
\newcommand{\GPS} {\textit{GPS}}
\newcommand{\ERS} {\textit{ERS}}
\newcommand{\GR} {\textit{GR}}
\newcommand{\IS} {\textit{IS}}
\newcommand{\ES} {\textit{ES}}
\newcommand{\zeroes}{\mathop{\textrm{zeroes}}}
\newcommand{\odd}{\mathop{\textrm{odd}}}
\newcommand{\even}{\mathop{\textrm{even}}}
\newcommand{\ff} {\textit{ff}}
\newcommand{\fm} {\textit{fm}}
\newcommand{\mf} {\textit{mf}}
\newcommand{\inv} {\textit{inv}}

\renewcommand{\zerob}{\mathbf{0}}
\renewcommand{\v}{\mathbf{v}}
\renewcommand{\u}{\mathbf{u}}
\newcommand{\w}{\mathbf{w}}
\renewcommand{\d}{\mathrm{d}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\Yvec}{\mathbf{Y}}
\newcommand{\Wvec}{\mathbf{W}}
\newcommand{\Gvec}{\mathbf{G}}
\newcommand{\Yt}{\widetilde{\mathbf{Y}}}
\newcommand{\Zvec}{\mathbf{Z}}
%\newcommand{\epsilonb}{\mbox{\boldmath{$\varepsilon$}}}
\newcommand{\epsilonb}{\boldsymbol{\varepsilon}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\renewcommand{\L}{\mathbf{L}}

\newcommand{\E}{\mathrm{E}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\Dist}{\mathrm{Dist}}
\renewcommand{\prec}{\mathrm{prec}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\trace}{\mathrm{tr}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\Gau}{\mathrm{Gau}}

\newcommand{\RR}{\mathbb{R}}

\newcommand{\s}{\mathbf{s}}
\newcommand{\p}{\mathbf{p}}
\renewcommand{\a}{\mathbf{a}}
\newcommand{\h}{\mathbf{h}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\z}{\mathbf{z}}


\newcommand{\blambda}{\boldsymbol{\lambda}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bzero}{\boldsymbol{0}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\bm{\Sigma}}

\newcommand{\zeros}{\textrm{zeros}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}


\bibliographystyle{apalike}

\title{Fixed Rank Kriging: The \tt{R} package}
\author{Andrew Zammit-Mangion}
%\author[1]{Noel Cressie}
%\affiliation{National Institute for Applied Statistics Research Australia~(NIASRA), School of Mathematics and Applied Statistics, University of Wollongong, New South Wales 2522, Australia}

\begin{document}


<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# set global chunk options
# opts_chunk$set(fig.path='figure/minimal-', fig.align='center', fig.show='hold')
# options(formatR.arrow=TRUE,width=90)
@
%\SweaveOpts{concordance=TRUE}

\maketitle

\begin{abstract}
Fixed Rank Kriging (FRK) is a spatial/spatio-temporal modelling and prediction framework designed for use with very large datasets. Although FRK is relatively straightforward to implement, and despite its extensive use in remote sensing contexts, to date very little software is available to facilitate this task. The present article discusses the development of a new \emph{R} package, \texttt{FRK}, that facilitiates FRK on the most commonly used manifolds ($\mathbb{R}^1, \mathbb{R}^2$ and $\mathbb{S}^2$), for both spatial and spatio-temporal fields. In particular, it provides automatic basis function generation, automatic basic areal unit construction, functionality to incorporate multiple datasets with different supports, an Expectation Maximisation (EM) algorithm for parameter estimation, and functionality for prediction over any user-specified area. Use of the package is illustrated on several small examples using both spatial and spatio-temporal datasets.
\end{abstract}

\tableofcontents

\newpage

\section{Introduction}

Fixed Rank Kriging (FRK) is a spatial/spatio-temporal modelling and prediction framework designed for use with large datasets. FRK hinges on the use of a Spatial Random Effects (SRE) model, in which a spatially correlated random spatial field is decomposed using a linear combination of basis functions with random coefficients. Dimensionality reduction ensures computationally-efficient prediction, while the reconstructed random spatial field is, in general, non-stationary.  The SRE model has a spatial covariance functions that is \emph{always} nonnegative-definite, and, because any (possibly non-orthogonal) basis functions can be used, it can be constructed so as to approximate standard families of covariance functions.  For a detailed treatment of FRK, see \cite{Cressie_2008}.



There are numerous \tt{R} packages available for modelling and prediction with spatial or spatio-temporal data,\footnote{see \tt{https://cran.r-project.org/web/views/Spatial.html}.} although relatively few of these make use of a model with spatial basis functions. A few variants of FRK have, however, been developed to date and, of these, the package \tt{LatticeKrig} \citep{Nychka_2015} deserves special mention. \tt{LatticeKrig} uses Wendland basis functions (that have compact support) to decompose the spatially correlated field, and a Markov assumption to construct a precision matrix  (the matrix $\Kmat^{-1}$ in Section \ref{sec:SREModel}) to describe the dependence between the coefficients of these basis functions. \tt{LatticeKrig} does not cater for what we term fine-scale process variation, and the finest scale of the process is limited to the finest resolution of the basis functions used (which, however, can be relatively high due to the imposed sparsity). \tt{LatticeKrig}'s model is also restricted to use precision matrices constructed using Gaussian Markov random field (GMRF) assumptions -- this results in efficient computations and the potential use of a large number ($>4000$) basis functions, but non-stationarity is limited in \tt{LatticeKrig's} GMRFs to either a few parameters that control spatially-varying variance or to some pre-determined specification \citep[][p.~21]{LatticeKrig}.  Heterogeneity is not straightforward in GMRFs, in general; for example inference on slowly varying length scales may be carried out using the approach of \citet{Lindgren_2011} but abrupt changes are difficult to characterise. % \red{There is one final feature of \tt{LatticeKrig} that allows fast computation but results in an incoherent spatial model. The same random effect can be assigned to two spatial resolutions, where it is assumed statistically independent of itself \citep{Nychka_2015,Bradley_206}. This can only happen in the degenerate case where the random effect is in fact non-random.}

In the standard flavour of \tt{FRK} \citep{Cressie_2008}, which we term \emph{vanilla} FRK, there is an explicit reliance on basis functions to give complex non-stationary patterns at the cost of not imposing a Markov random-field assumption. The number of basis functions in vanilla \tt{FRK} is thus bounded above by a couple of thousand and thus should be carefully chosen. This package is concerned with the vanilla approach to FRK in \tt{R}; some unpackaged MATLAB code is available at \tt{http://niasra.uow.edu.au/cei/webprojects/UOW175995.html}.

Other general purpose packages, such as \tt{INLA}, also have the functionality that FRK requires. When using \tt{INLA}, the usual approach is to assume that basis functions are `tent' functions, and that the distribution of the coefficients are Gaussian with a sparse precision matrix, such that the covariance function of the resulting Gaussian field is approximately one from the Mat{\'e}rn class of spatial covariance functions \citep[see][for details on software implementation]{Lindgren_2015}. \tt{INLA}'s approach thus shares many of the advantages and limitations of \tt{LatticeKrig}. Vanilla FRK can also be implemented with \tt{INLA}; this would, however, require considerable extra implementation effort, especially if one is using spatio-temporal models. The advantage of \tt{INLA} is that once the basic model is constructed, one has access to all the approximate-inference machinery and likelihood models available within the package.

The predictive process approach of \citet{Banerjee_2008} can also be seen as a type of Bayesian FRK, where the basis functions are constructed from the postulated covariance function of the spatial random effects and hence depend on parameters \citep[see][for an equivalence argument]{Katzfuss_2014}. An \tt{R} package that implements predictive processes is \tt{spBayes}. \tt{spBayes} allows for multivariate spatial or spatio-temporal fields, and both spatial/spatio-temporal processes  and Bayesian inference is carried out using Markov chain Monte Carlo (MCMC), allowing for a variety of likelihood models. Because basis functions are constructed based on a prespecified parametric covariance model, they change at each MCMC iteration which results in some computational overhead.

The aim of the package \tt{FRK} is to considerably facilitate spatial and spatio-temporal analysis based on what, from experience, we deem to be the most `usual' scenario faced by the analyst in the environmental sciences:
\begin{itemize}
\item The data has additive Gaussian measurement error, and is available at multiple spatial resolutions (or supports).
\item The physical process being modelled is either spatial or spatio-temporal, on $\mathbb{R}^1, \mathbb{R}^2$ or $\mathbb{S}^2$, and can be heterogeneous in both space and time.
\item There is a spatial (or spatio-temporal) unit, which we term a Basic Areal Unit (BAU), which is the smallest unit over which an observation is considered informative or the smallest unit over which we need to predict.
\item Datasets can have hundreds of thousands of data points and hundreds of thousands of BAUs.
\item The extent, or nature, of spatial heterogeneity can vary greatly between one application and the next. A useful class of models for the analyst would thus be  one which is quite basic but that can also offer considerably flexibility (such as the SRE class).
\end{itemize}

The package \tt{FRK} meets the needs of a spatial statistical analyst by requiring a consistent, straightforward sequence of commands for the analysis, irrespective of whether the multiple datasets in use have different supports or not, irrespective of the manifold being used, irrespective of whether or not a temporal dimension needs to be included or not, and irrespective of the `prediction resolution'. The package is thus designed to be relatively easy to use, with a straightforward model (outlined in Section \ref{sec:theory}), and with the capability of handling large datasets on a standard desktop machine (up to a few hundreds of thousands of data points).

The package requires the user to follow the same six steps in each analysis:

\begin{itemize}
\item {\bf Step 1:} Place the data into a format understood by the packages \tt{sp} or \tt{spacetime}, that is, either \tt{SpatialPointsDataFrame} or \tt{STIDF} for point-referenced data, and \tt{SpatialPolygonsDataFrame} for polygon-referenced data.
\item {\bf Step 2:} Construct a prediction grid of \emph{Basic Areal Units} (BAUs) using \tt{auto\_BAUs}, where each BAU is representative of the smallest scale on which we wish to carry out inference (the process is averaged within each BAU). The BAUs are of class \tt{SpatialPolygonsDataFrame} for spatial problems, and of class \tt{STFDF} for spatio-temporal problems.
\item {\bf Step 3:} Construct a set of regularly or irregularly spaced basis functions using \tt{auto\_basis}. The basis functions can be of various types (e.g., based on bisquare functions) and can be automatically pruned in regions of sparse data.
\item {\bf Step 4:} Construct an SRE model using \tt{SRE} from an \tt{R} formula that identifies the response variable and the covariates, the data, the BAUs, and the basis functions.
\item {\bf Step 5:} Estimate the parameters within the SRE model using \tt{SRE.fit}. Estimation is carried out using the Expectation Maximisation (EM) algorithm.
\item {\bf Step 6:} Predict either at the BAU level, or over arbitrary polygons made up of BAUs specified as \tt{SpatialPolygon}s or \tt{SpatialPolygonDataFrame}s, using \tt{SRE.predict}.
\end{itemize}

In this vignette we illustrate these six steps, and thus show the versatility of \tt{FRK}, by applying them to both spatial and spatio-temporal datasets with differing supports and on different manifolds. In Section 2 we first present the model, the estimation approach and the prediction equations. In Sections 3 and 4 we consider examples of spatial and spatio-temporal data, respectively. In Section 5 we discuss some additional functionality (e.g., modelling of anisotropic fields) and in Section 6 we discuss package limitations and opportunities for further development.

\section{Outline of Fixed Rank Kriging: Modelling, estimation and prediction} \label{sec:theory}

In this section we present the theory behind the operations in \tt{FRK}. In Section \ref{sec:SREModel} we introduce the SRE model, in Section \ref{sec:estimation} we discuss the EM algorithm for parameter estimation, and in Section \ref{sec:prediction} we present the prediction equations.

\subsection{The SRE model} \label{sec:SREModel}

Denote the spatial field of interest as $Y(\svec)$, where $\svec \in D$ and $D$ is our domain of interest. In the following, we assume that $D$ is a spatial domain but extensions to spatio-temporal domains are natural within the framework; these are considered in Section \ref{sec:spacetime}. The SRE model is
\begin{equation}
Y(\svec) = \tvec(\svec)'\alphab + \upsilon(\svec) + \delta(\svec), \quad \svec \in D,
\end{equation}
where $\tvec(\svec)$ is a vector of spatially-referenced covariates, $\alphab$ are regression coefficients, $\upsilon(\svec)$ is a small-scale, spatially-correlated random effect, and $\delta(\svec)$ is a fine-scale random effect. In order to cater for different observation supports, it is convenient to assume a domain of interest $D^L \equiv \{A_i \subset D: i = 1,\dots,N\}$ that contains $N$ small, non-overlapping BAUs \citep{Nguyen_2012}. The domain $D^L$ could be thought of as a discretisation of the original domain $D$. The process averaged over the BAUs is then the vector $\Yvec = (Y_i : i = 1,\dots,N)'$, where

\begin{equation}
Y_i \equiv \frac{1}{|A_i|}\int_{A_i} Y(\svec) \intd \svec, \quad i = 1,\dots,N,
\end{equation}

\noindent and $N$ is the number of BAUs. At this BAU level,
\begin{equation} \label{eq:Yi1}
Y_i = \tvec_i'\alphab + \upsilon_i + \delta_i,
\end{equation}
\noindent where $\tvec_i \equiv \frac{1}{|A_i|}\int_{A_i} \tvec(\svec) \intd \svec$, $\upsilon_i\equiv \frac{1}{|A_i|}\int_{A_i} \upsilon(\svec) \intd \svec$, and $\delta_i$ is specified lated.
In FRK, it is assumed that the spatial random effect $\upsilon(\svec)$ can be decomposed using basis functions, $\phib(\svec) \equiv (\phi_i(\svec): i = 1,\dots,r )'$, where $r > 0$ is the number of basis functions. Generally, $\phib(\svec)$ is multiresolutional and its elements may have compact support. The basis chosen should be able to adequately reconstruct realisations of $Y(\svec)$; an empirical spectral-based approach that can ensure this is discussed in \cite{Zammit_2012}. Using basis-function decomposition, we let $\upsilon(\svec) = (\phi(\svec),\dots,\phi(\svec))\etab \equiv \phib(\svec)'\etab$. Then, $\upsilon_i = \frac{1}{|A_i|}\int_{A_i}\phib(\svec)'\intd \svec ~\etab, i = 1,\dots,N$, so that $\upsilonb = \Smat\etab$, where $\Smat$ is the $N \times r$ matrix
\begin{equation}\label{eq:S}
\Smat \equiv \left(\frac{1}{|A_i|}\int_{A_i}\phib(\svec)\intd\svec : i = 1,\dots,N\right)'.
\end{equation}
We assume that $\etab$ is $r$-dimensional Gaussian with mean zero and $r \times r$ covariance matrix $\Kmat$, which needs to be estimated. Frequently, the BAUs are sufficiently small, and the basis functions sufficiently smooth, so that
\begin{equation}\label{eq:Sapprox}
\Smat \approx \left(\phib(\svec_i) : i = 1,\dots,N\right)',
\end{equaiton}
where $\{\svec_i: i = 1,\dots,N\}$ are the centroids of the BAUs. This approximation can be used to simplify computations when multiple, small, BAUs are used.

In \tt{FRK}, we assume that the BAU is the smallest unit of measure. Thus, the fine-scale variation is a constant random effect in each BAU, but uncorrelated across the BAUs. Since a BAU has finite area, the fine-scale variation is not a nugget effect in the classical sense. We thus model $\delta_i$ to be Gaussian with mean zero and uncorrelated with variance
\begin{equation}
\var(\delta_i) = \sigma^2_\delta \varv_i,
\end{equation}
where $\sigma^2_\delta$ is a parameter that needs to be estimated, and $(\varv_1,\dots,\varv_N)$  represent known heteroscedasticity. From \eqref{eq:Yi1}, we can write
\begin{equation}\label{eq:SRE_Y}
\Yvec = \Tmat\alphab + \Smat\etab + \deltab,
\end{equation}
where $\Tmat \equiv (\tvec_i: i = 1,\dots,N)'$, $\deltab \equiv (\delta_i : i = 1,\dots,N)'$, and $\var(\deltab) \equiv \sigma^2_\delta \Vmat$, where $\Vmat \equiv \diag(\varv_1,\dots,\varv_N)$ and known.

We now  assume that the unobserved field, $Y(\svec)$, is observed with $m$ footprints spanning one or more BAUs. We thus define the observation domain as $D^O \subset \{ \cup_{i \in c_j} A_i : c_j \in 2^{\{1,2,\dots,N\}} \setminus \varnothing, j = 1,\dots,m \}$, where $2^{\{1,2,\dots,N\}}$ is the power set of $\{1,2,\dots,N\}$. For illustration, consider the simple case of three BAUs. Then $D^L = \{A_1,A_2,A_3\}$ and, for example, $D^O = \{B_1, B_2\} = \{A_1 \cup A_2,A_3\}$, in which case $B_1 = A_1 \cup A_2$ and $B_2 = A_3$. Catering for different footprints is important for remote sensing applications in which satellite instrument footprints can differ widely \citep[e.g.,][]{Zammit_2015}.

Each $B_j \in D^O$ is either a BAU, or a union of BAUs. The measurement process is thus given by
\begin{equation}
Z_j \equiv Z(B_j) = \left(\frac{1}{|B_j|} \sum_{i : A_i \subset B_j}Y_i\right) + \epsilon_j; \quad B_j \in D^O, A_i \in D^L,
\end{equation}
where $\epsilon_j$ is Gaussian measurement error.  We assume that the observations are conditionally independent, when conditioned on $\Yvec$. Therefore $\{\epsilon_j: j = 1,\dots,m\}$, where $m \equiv |D^O|$ is the number of observations, are independent. If these errors are also identically distributed, then $\var(\epsilon_j), j = 1,\dots,m$ can in principle be estimated from the data (provided $\Vmat$ is not proportional to the identity matrix, since otherwise it is confounded with the fine-scale variation).

Let $\Zvec \equiv (Z_j : j = 1,\dots,m)'$. Then, since each element in $D^O$ is the union of subsets of $D^L$, one can construct a matrix $\Cmat_Z$, the rows of which sum to one, such that
\begin{equation}
\Zvec = \Cmat_Z\Yvec + \epsilonb,
\end{equation}
\noindent where $\epsilonb \equiv (\epsilon_j : j = 1,\dots,m)'$ and $\var(\epsilonb) = \Sigmamat_\epsilon$ is a diagonal covariance matrix. It will be convenient, as a consequence of conditional independence, to re-write
\begin{equation}\label{eq:Z_collapsed}
\Zvec = \Tmat_Z\alphab + \Smat_Z\etab + \deltab_Z + \epsilonb,
\end{equation}
where $\Tmat_Z \equiv \Cmat_Z \Tmat$, $\Smat_Z \equiv \Cmat_Z \Smat$, $\deltab_Z \equiv \Cmat_Z \deltab$ and $\var(\deltab_Z) = \sigma^2_\delta\Vmat_Z \equiv \sigma^2_\delta\Cmat_Z\Vmat\Cmat_Z'$. In practice, it is not always possible for each $B_i$ to overlap a BAU exactly. For simplicity, we assume that the observation footprint overlaps a BAU only if the BAU centroid lies within the footprint. Frequently, data is also supplied as point-referenced. In this case each data point is attributed to a specific BAU and it is possible to have multiple observations of the same BAU.


We collect the unknown parameters in the set $\thetab \equiv \{\alphab, \sigma^2_\delta, \Kmat\}$; their estimation is the subject of Section \ref{sec:estimation}. Once the parameters in $\thetab$ are estimated, a straightforward inversion may then be used to predict over unknown areas. In \texttt{FRK} we allow the prediction set $D^P$ to be as flexible as $D^O$; specifically, $D^P \subset \{ \cup_{k \in c_k} A_i : c_k \in 2^{\{1,2,\dots,N\}} \setminus \varnothing, k = 1,\dots,N_P \}$, where $N_P$ is the number of prediction areas. We can thus predict both at the individual BAU level or averages over an area spanning multiple BAUs, and these prediction regions may overlap. We provide the prediction equations in Section \ref{sec:prediction}.

\subsection{Parameter estimation using an EM algorithm} \label{sec:estimation}

We carry out parameter estimation using an EM algorithm \citep{Katzfuss_2011,Nguyen_2014} with \eqref{eq:Z_collapsed} as our model. Define the \emph{complete-data} likelihood $L_c(\thetab) \equiv [\etab,\Zvec \mid \thetab]$ (with $\deltab_Z$ integrated out), where $[\cdot]$ denotes the probability distribution of its argument. In the E-step, the function
\begin{equation}
Q(\thetab \mid \thetab^{(l)}) \equiv \E(\ln L_c(\thetab) \mid \Zvec,\thetab^{(l)}),
\end{equation}
is found for some current estimate $\thetab^{(l)}$. In the M-step, the updated parameter estimate
\begin{equation}
\thetab^{(l+1)} = \argmax_\thetab Q(\thetab \mid \thetab^{(l)}),
\end{equation}
is found. It is straightforward to show that conditionally,
\begin{equation}
\etab \mid \Zvec,\thetab^{(l)} \sim \Gau(\muvec_\eta^{(l)},\Sigmamat_\eta^{(l)}),
\end{equation}
where
\begin{align}
\muvec_\eta^{(l)} &= \Sigmamat_\eta^{(l)} \Smat_Z'(\Dmat_Z^{-1})^{(l)}(\Zvec - \Tmat_Z\alphab^{(l)}), \\
\Sigmamat_\eta^{(l)} &= (\Smat_Z'(\Dmat_Z^{-1})^{(l)}\Smat_Z + (\Kmat^{-1})^{(l)})^{-1},
\end{align}
and where $\Dmat_Z^{(l)} \equiv (\sigma^2_\delta)^{(l)} \Vmat_Z + \Sigmamat_\epsilon$.

The updates for $\alphab^{(l+1)}$ and $\Kmat^{(l+1)}$ are
\begin{align}
\alphab^{(l+1)} &= (\Tmat_Z' (\Dmat_Z^{-1})^{(l+1)} \Tmat_Z)^{-1}\Tmat_Z'(\Dmat_Z^{-1})^{(l+1)}(\Zvec - \Smat_Z \muvec_\eta^{(l)}), \label{eq:alpha}\\
\Kmat^{(l+1)} &= \Sigmamat_\eta^{(l)} + \muvec_\eta^{(l)} \muvec_\eta^{(l)'},
\end{align}
while that for $\sigma_\delta^{2^{(l+1)}}$ requires the solution to
\begin{equation} \label{eq:sigma2d}
\tr((\Sigmamat_{\epsilon} + (\sigma^2_\delta)^{(l+1)}\Vmat_Z)^{-1}\Vmat_Z) = \tr((\Sigmamat_{\epsilon} + (\sigma^2_\delta)^{(l+1)}\Vmat_Z)^{-1}\Vmat_Z(\Sigmamat_{\epsilon} + (\sigma^2_\delta)^{(l+1)}\Vmat_Z)^{-1}\Omegab),
\end{equation}
where
\begin{equation} \label{eq:Omegab}
\Omegab \equiv \Smat_Z \Sigmamat_\eta^{(l)} \Smat_Z' + \Smat_Z \muvec_\eta^{(l)}\muvec_\eta^{(l)'} \Smat_Z' - 2\Smat_Z\muvec_\eta^{(l)}(\Zvec - \Tmat_Z\alphab^{(l+1)})' + (\Zvec - \Tmat_Z\alphab^{(l+1)})(\Zmat - \Tmat_Z\alphab^{(l+1)})'.
\end{equation}

\noindent The solution to \eqref{eq:sigma2d} is found using a standard root-finding algorithm after the expression for $\alphab^{(l+1)}$ is substituted into \eqref{eq:Omegab}. After $ (\sigma^2)^{(l+1)}$ is found, $\alphab^{(l+1)}$ is found from \eqref{eq:alpha}. Computational simplifications are possible when $\Vmat_Z$ and $\Sigmamat_\epsilon$ are diagonal (that is, when observations do not overlap spatially or spatio-temporally are are mutually uncorrelated) since then only the diagonal of $\Omegab$ needs to be computed. Further simplifications are possible when $\Vmat_Z$ and $\Sigmamat_\epsilon$ are proportional to the identity matrix, with constants of proportionality $c_1$ and $c_2$, respectively. In this case
\begin{equation}
(\sigma^2_\delta)^{(l+1)} = \frac{1}{c_1} \left(\frac{\tr(\Omegab)}{m} - c_2 \right),
\end{equation}
and $\alphab^{(l+1)}$ is simply the ordinary least squares estimate given $\muvec_\eta^{(l)}$. These simplifications are used by $\texttt{FRK}$ when possible.

Convergence of the EM algorithm is assessed using the (\emph{incomplete-data}) log-likelihood function at each iteration,
\begin{equation}
\ln [p(]\Zvec \mid \alphab^{(l)}, \Kmat^{(l)}, (\sigma^2_\delta)^{(l)}] = -\frac{m}{2}\ln 2\pi -\frac{1}{2}\ln |\Sigmamat_Z^{(l)}| - \frac{1}{2}(\Zvec - \Tmat_Z\alphab^{(l)})'\Sigmamat_Z^{-1^{(l)}}(\Zvec - \Tmat_Z\alphab^{(l)}),
\end{equation}
where
\begin{equation}
\Sigmamat_Z^{(l)} = \Smat_Z \Kmat^{(l)} \Smat_Z' + \Dmat_Z^{(l)},
\end{equation}
and recall that $\Dmat_Z^{(l)} \equiv (\sigma_\delta^2)^{(l)}\Vmat_Z + \Sigmamat_\epsilon$. Efficient computation of the log-likelihood is facilitated through the use of the Sherman-Woodbury matrix identity and a matrix-determinant lemma \citep[e.g.,][]{Cressie_2008}. Specifically, the operations
\begin{align}
(\Sigmamat_Z^{-1})^{(l)} &= (\Dmat_Z^{-1})^{(l)} - (\Dmat_Z^{-1})^{(l)} \Smat_Z ((\Kmat^{-1})^{(l)} + \Smat'_Z (\Dmat_Z^{-1})^{(l)}\Smat_Z)^{-1}\Smat_Z'(\Dmat_Z^{-1})^{(l)},\\
| \Sigmamat_Z^{(l)}  | &= | (\Kmat^{-1})^{(l)} + \Smat_Z'(\Dmat_Z^{-1})^{(l)} \Smat_Z | |\Kmat^{(l)} | |\Dmat_Z^{(l)} |,\label{eq:determinant}
\end{align}
ensure that we only deal with vectors of length $m$ and matrices of size $r \times r$, where typically $r \ll m$. To prove \eqref{eq:determinant}, start from the right-hand-side, noting that $| \Sigmamat_Z^{(l)}|  = | \Imat + \Smat_Z'(\Dmat_Z^{-1})^{(l)} \Smat_Z \Kmat^{(l)}| |\Dmat_Z^{(l)} |.$ Applying Sylvester's determinant identity, $| \Imat + \Smat_Z'(\Dmat_Z^{-1})^{(l)} \Smat_Z \Kmat^{(l)}| \equiv | \Imat +  \Smat_Z \Kmat^{(l)}\Smat_Z'(\Dmat_Z^{-1})^{(l)}|$, we see that $| \Sigmamat_Z^{(l)}| =  |\Smat_Z \Kmat^{(l)} \Smat_Z' + \Dmat_Z^{(l)}|$, as required.

\subsection{Prediction} \label{sec:prediction}

They key object of prediction is to estimate $Y(\svec)$ or $(Y(\svec) - \delta(\svec))$ (i.e., when one wishes to exclude the small-scale variation of the field) over $D^P$. Consider the process $Y_P(B_i)$, which, similar to the observations, is constructed using the BAUs $\{A_i: i = 1,\dots,N\}$:

\begin{equation}
Y_{P,i} \equiv Y_{P}(B_i) = \frac{1}{|B_i|} \sum_{i : A_i \subset B_i}Y_i, \quad B_i \in D^P.
\end{equation}
Let $\Yvec_P \equiv (Y_{P,i} : i = 1,\dots,N_P)'$, where $N_P$ is the number of prediction areas and is also equal to $|D^P|$. Then, since each element in $D^P$ is the union of subsets of $D^L$, one can construct a matrix $\Cmat_P$, the rows of which sum to one, such that
\begin{equation}
\Yvec_P = \Cmat_P\Yvec = \Tmat_P\alphab + \Smat_P\etab + \deltab_P,
\end{equation}
where $\Tmat_P \equiv \Cmat_P \Tmat$, $\Smat_P \equiv \Cmat_P \Smat$, $\deltab_P \equiv \Cmat_P \deltab$ and $\var(\deltab_P) = \sigma^2_\delta\Vmat_P \equiv \sigma^2_\delta\Cmat_P\Vmat\Cmat_P'$.  As with the observations, these prediction regions may overlap. In practice, it is not always possible for each $B_i$ to overlap a BAU directly. In this case, we assume that the prediction region overlaps a BAU only if the BAU centroid lies within the prediction area.

Let $l^*$ denote the EM iteration number at which convergence was reached. The final estimates are then $\widehat\muvec_\eta = \muvec_\eta^{(l^*)}, \widehat\Sigmamat_\eta = \Sigmamat_\eta^{(l^*)}, \widehat\alpha = \alpha^{(l^*)}, \widehat\Kmat = \Kmat^{(l^*)}$ and $\widehat\sigma^2_\delta = (\sigma^2_\delta)^{(l^*)}$. The prediction vector and covariance matrix corresponding to the smooth field, $\Yvec_P - \deltab_P,$ are
\begin{align}
\E(\Yvec_P - \deltab_P \mid \Zvec) &= \Tmat_P\widehat\alphab + \Smat_P\widehat\muvec_\eta, \label{eq:smooth1}\\
\var(\Yvec_P - \deltab_P \mid \Zvec) &= \Smat_P \widehat\Sigmamat_\eta\Smat_P'. \label{eq:smooth2}
\end{align}
However, in order to predict $\Yvec_P$ we also need to predict $\deltab_P$, which is not available at this stage. This quantity is not necessarily identical to $\deltab_Z$; this is why $\deltab_Z$ was not estimated in Section \ref{sec:estimation}.

To cater for arbitrary observation and prediction support, we predict $\Yvec_P$ by first carrying out prediction over $\Yvec$, that is, at the BAU level, and then transforming linearly through $\Cmat_P$. Let $\Wvec \equiv (\etab',\deltab')'$ and $\Pimat \equiv (\Smat,\Imat)$. Then \eqref{eq:SRE_Y} can be re-written as $\Yvec = \Tmat\alphab + \Pimat\Wvec$ and
\begin{align}
\widehat\Yvec \equiv \E(\Yvec \mid \Zvec) &= \Tmat\widehat\alphab + \Pimat\widehat\Wvec, \\
\Sigmamat_{Y \mid Z} \equiv \var(\Yvec \mid \Zvec) &= \Pimat \Sigmamat_W\Pimat', \label{eq:Sigma_YZ}
\end{align}
where
\begin{align}
\Sigmamat_W &\equiv (\Pimat' \Cmat_Z' \Sigmamat_\epsilon^{-1} \Cmat_Z \Pimat + \Lambdamat^{-1})^{-1}, \label{eq:Winv}\\
\widehat\Wvec &\equiv \Sigmamat_W\Pimat'\Cmat_Z'\Sigmamat_\epsilon^{-1}(\Zvec - \Tmat_Z\widehat\alphab),\label{eq:What}
\end{align}
and the block diagonal matrix $\Lambdamat \equiv \textrm{bdiag}(\widehat\Kmat,\widehat\sigma^{2}_\delta\Vmat_P)$, where $\textrm{bdiag}(\cdot)$ returns a block diagonal matrix of its arguments. It then follows that $\E(\Yvec_P \mid \Zvec) = \Cmat_P\widehat\Yvec$ and
\begin{equation}\label{eq:YvecP}
\var(\Yvec_P \mid \Zvec) = \Cmat_P \Sigmamat_{Y \mid Z} \Cmat_P'.
\end{equation}
By carrying out all intensive operations on small, dense matrices, equations up to this point all have moderate computational and memory complexity. On the other hand, \eqref{eq:YvecP} indicates that all of $\Sigmamat_{Y \mid Z}$ (and hence $\Sigmamat_W$) needs to be computed and stored. In reality, only a few elements of $\Sigmamat_{Y \mid Z}$ need to be known and thus computational simplifications are possible. However, the larger the aggregation implied by $\Cmat_P$, the more the elements that need to be computed  in $\Sigmamat_{Y \mid Z}$ and the slower the prediction.

\section{Fixed Rank Kriging on $\mathbb{R}^2$ or $\mathbb{S}^2$}

In this part of the vignette we apply \texttt{FRK} to the case when we have spatial data, either on the plane or on the surface of a sphere. For 2D data on the plane, we consider the \texttt{meuse} data, which can be found in the package \texttt{sp}. For data on the sphere we will use readings taken between May 01 2003 and May 03 2003 (inclusive) by the Atmospheric InfraRed Sounder (AIRS) on board the Aqua satellite \citep[e.g.,][]{Chahine_2006}. For spatial modelling of the data we need to load the following packages
<<eval=TRUE,message=FALSE>>=
library(sp)        # for defining points/polygons
library(ggplot2)   # for plotting
library(dplyr)     # for easy data manipulation
library(FRK)       # for carrying out FRK
@

\noindent and, to keep the document tidy, we will set the \texttt{progress} package option to \texttt{FALSE}. Parallelisation is frequently used in \tt{FRK}, but for the purposes of this document we will set the \texttt{parallel} option to 0 as well.

<<eval=TRUE>>=
opts_FRK$set("progress",FALSE)  # no progress bars
opts_FRK$set("parallel",0L)     # no parallelisation
@

\noindent The mesher in \tt{INLA} is also required for this vignette. Please install \tt{INLA} by visiting \tt{http://www.r-inla.org/download}.

\subsection{The \tt{meuse} dataset} \label{sec:meuse}

The \tt{meuse} dataset contains readings of heavy-metal abundance in a region of The Netherlands along the river Meuse. For more details on the dataset see the vignette titled `gstat' in the package \tt{gstat}. The aim of this vignette is to analyse the spatial distribution of zinc-concentration from spatially sparse readings using FRK.

\vspace{0.1in}

\noindent {\bf Step 1:} We first load the \tt{meuse} data:

<<>>=
data(meuse)            # load meuse data
print(class(meuse))    # print class of meuse data
@
\noindent The \texttt{meuse} data is of class \texttt{data.frame}. However, \texttt{FRK} needs all spatial objects to be of class \texttt{SpatialPointsDataFrame} or \texttt{SpatialPolygonsDataFrame}, depending on whether the dataset is point-referenced of area-referenced. The \tt{meuse} data is point referenced, and we therefore cast it into a \texttt{SpatialPointsDataFrame} by applying the \tt{coordinates} function as follows:

<<>>=
coordinates(meuse) = ~x+y     # change into an sp object
@

\vspace{0.1in}

\noindent {\bf Step 2:} Based on the data we now generate BAUs. For this, we can use the helper function \tt{auto\_BAUs}:

<<message=FALSE>>=
set.seed(1)
GridBAUs1 <- auto_BAUs(manifold = plane(),     # 2D plane
                     cellsize = c(100,100),   # BAU cellsize
                     type = "grid",           # grid (not hex)
                     data = meuse,            # data around which to create BAUs
                     convex=-0.05)            # border buffer factor
@

\noindent The \tt{auto\_BAUs} function takes several arguments (see \tt{help(auto\_BAUs)} for details). Above, we instruct the helper function to construct BAUs on the plane, centred around the data \tt{meuse} with each BAU of size 100 $\times$ 100 (with units in m since the data is supplied with x-y coordinates in m). The \tt{type="grid"} input instructs that we want a rectangular grid and not a hexagonal lattice (use \tt{"hex"} for a hexagonal lattice), and \tt{convex=-0.05} is a specific parameter controlling the spatial-domain boundary (see \tt{INLA::inla.nonconvex.hull} for more details). For the $i$-th BAU, we also need to attribute the element $\varv_i$ that describes the hetereoscedascity of the fine-scale variation for that BAU. As described in Section \ref{sec:SREModel}, this component encompasses all process variation that occurs at the BAU scale and only needs to be known up to a constant of proportionality, $\sigma^2_\delta$; this constant is estimated using maximum likelihood with \tt{SRE.predict} using the EM algorithm of Section \ref{sec:estimation}. Typically, geographic features such as altitude are appropriate, but in this case we will just set this parameter to unity. It is important that this field is labelled `fs':

<<>>=
GridBAUs1$fs <- 1   # fine-scale variation at BAU level
@
\noindent The data and BAUs are illustrated using the \tt{plot} function in Fig.~\ref{fig:meuse}.

<<echo=FALSE,fig.cap="(a) Locations of the \\tt{meuse} data. (b) BAUs for Fixed Rank Kriging with the \\tt{meuse} dataset.\\label{fig:meuse}",fig.subcap=c("",""),fig.width=5,fig.height=4,out.width="0.5\\linewidth",fig.align='center'>>=
plot(NULL,NULL,xlim = c(178605,181390),ylim=c(329714,333611),asp=1,xlab="Easting (m)",ylab="Northing (m)")
plot(meuse,add=T)
plot(NULL,NULL,xlim = c(178605,181390),ylim=c(329714,333611),asp=1,xlab="Easting (m)",ylab="Northing (m)")
plot(GridBAUs1,add=T)
@

\vspace{0.1in}

\noindent {\bf Step 3:} \tt{FRK} decomposes the spatial process as a sum of basis functions that may either be user-specified (see Section \ref{sec:custom_basis}) or constructed using helper functions. To create spatial basis functions we use the helper function \tt{auto\_basis} as follows:

<<>>=
G <- auto_basis(m = plane(),          # 2D plane
                data=meuse,           # meuse data
                nres = 2,             # number of resolutions
                prune=5,              # prune threshold
                type = "Gaussian",    # type of basis function
                regular = 0)          # place irregularly in domain
@

\noindent The argument \tt{nres = 2} indicates how many resolutions we wish, while \tt{type = "Gaussian"} indicates that the basis set we want is composed of Gaussian functions. Other built-in functions that can be used are \tt{"exp"} (the exponential covariance function), \tt{"bisquare"} (the bisquare function), and \tt{"Matern32"} (the Mat{\'e}rn covariance function with smoothness parameter equal to 1.5). Basis functions do not have to be positive-definite and are constructed such that the process is accurately represented where data is present, and not represented otherwise. This is controlled by the parameter \tt{prune}, see \tt{help(auto\_basis)} for details. The basis can be visualised using \tt{show\_basis}, see Fig.~\ref{fig:basis}.


<<fig.cap="Basis functions automatically generated for the meuse dataset with 2 resolutions. The interpretation of the circles change with the domain and basis. For Gaussian functions on the plane, each circle is centred at the basis function centre, and has a radius equal to $1\\sigma$. Type \\tt{help(auto\\_basis)} for details.\\label{fig:basis}.",fig.height=4,fig.width=4,fig.align='center',fig.pos="t">>=
show_basis(G) +             # illustrate basis functions
    coord_fixed() +         # fix aspect ratio
    xlab("Easting (m)") +   # x-label
    ylab("Northing (m)")    # y-label
@

\vspace{0.1in}

\noindent {\bf Step 4:} With the BAUs and the basis functions specified, we can construct the SRE model. For fixed effects, we just use an intercept; if we wish to use covariates, one must make sure that they are also specified at the BAU level (and hence attributed to \tt{GridBAUs1}). The fixed effects are supplied in a usual \tt{R} formula, which we store in \tt{f}:

<<>>=
f <- log(zinc) ~ 1    # formula for SRE model
@

\noindent The Spatial Random Effects model is then constructed using the function \tt{SRE}, which essentially bins the data in the BAUs, constructs all the matrices required for estimation, and provides initial guesses for the quantities that need to be estimated.


<<>>=
S <- SRE(f = f,                  # formula
         data = list(meuse),     # list of datasets
         BAUs = GridBAUs1,       # BAUs
         basis = G,              # basis functions
         est_error=TRUE,         # estimation measurement error
         average_in_BAU = FALSE) # do not average data over BAUs
@

\noindent The function \tt{SRE} takes as arguments the formula; the data (as a list that can include additional datasets); the BAUs; the basis functions; a flag; \tt{est\_error}; and a flag \tt{average\_in\_BAU}. The flag \tt{est\_error} indicates whether we wish to attempt to estimate the measurement-error variance $\Sigmamat_\epsilon \equiv \sigma^2_\epsilon\Imat$ or not using variogram methods \citep{Kang_2009}. Currently, \tt{est\_error = TRUE} is only allowed with spatial data. When not set, the dataset needs to also contain a field \tt{std}, the standard deviation of the measurement error.

In practice, several datasets (such as the \tt{meuse} dataset) are point-referenced. Since \tt{FRK} is built on the concept of a Basic Areal Unit, the smallest footprint of an observation has to be equal to that of a BAU. If multiple point-referenced observations fall within the same BAU, then these are assumed to be readings of the same random variable (hence, the fine-scale variation is not a nugget in the classical sense).  When multiple data points can fall into the same BAU, the matrix $\Vmat_Z$ is not diagonal; this increases computational time considerably. For large point-referenced datasets, such as the \tt{AIRS} dataset considered in Section \ref{sec:AIRS}, one can use the argument \tt{average\_in\_BAU = TRUE} to indicate that one wishes to summarise the data at the BAU level. When this flag is set, all data falling into one BAU is averaged; the measurement error of the averaged data point is then taken to be the average measurement error of the individual data points (i.e., measurement error is assumed to be perfectly correlated within the BAU). Consequently, the dataset is thinned; this can be used to obtain quick results prior to a more detailed analysis.

\vspace{0.1in}

\noindent {\bf Step 5:} The SRE model is fitted using the function \tt{SRE.fit}. Maximum likelihood is carried out using the EM algorithm of Section \ref{sec:estimation}, which is assumed to have converged either when \tt{n\_EM} is exceeded, or when the likelihood across subsequent steps does not change by more than \tt{tol}. In this example, the EM algorithm converged in 100 iterations; see Fig. \ref{fig:EM}.

<<cache=FALSE,fig.cap="Convergence of the EM algorithm when using \\tt{FRK} with the \\tt{meuse} dataset.\\label{fig:EM}",fig.height=4,fig.width=5,fig.align='center'>>=
S <- SRE.fit(SRE_model = S,    # SRE model
             n_EM = 400,       # max. no. of EM iterations
             tol = 0.01,       # tolerance at which EM is assumed to have converged
             print_lik=TRUE)   # print log-likelihood at each iteration
@

\vspace{0.1in}

\noindent {\bf Step 6:} Finally, we predict at all the BAUs with the fitted model. This is done using the function \tt{SRE.predict}. The argument \tt{use\_centroid} dictates whether we invoke the approximation \eqref{eq:Sapprox} or not, see \tt{help(SRE.predict)} for details.

<<>>=
GridBAUs1 <- SRE.predict(SRE_model = S,          # SRE model
                        use_centroid = TRUE)    # use centroid as point reference
@

\noindent The object \tt{GridBAUs1} now contains the prediction vector and the square of the prediction standard error at the BAU level in the fields \tt{mu} and \tt{var}, respectively. These can be plotted using the standard plotting commands, such as those in \tt{sp} or \tt{ggplot2}. To use the latter, we first need to convert the \tt{Spatial} object to a data frame as follows:

<<>>=
BAUs_df <- SpatialPolygonsDataFrame_to_df(sp_polys = GridBAUs1,   # BAUs to convert
                                          vars = c("mu","var"))  # fields to extract
@


\noindent The function \tt{SpatialPolygonsDataFrame\_to\_df} takes as argument the BAUs and the variables we wish to extract from the BAUs. Now \tt{ggplot2} can be used to plot the observations, the predictions, and the standard errors; for example, the following code yields the plots in Fig.~\ref{fig:PredictionBAU}.

<<>>=
g1 <- LinePlotTheme() +                          # Use a plain theme
    geom_polygon(data=BAUs_df ,                  # Draw BAUs
                 aes(x,y,fill=mu,group=id),      # Colour <-> Mean
                 colour="light grey") +          # Border is light grey
    scale_fill_distiller(palette="Spectral")  +  # Spectral palette
    geom_point(data=data.frame(meuse),           # Plot data
               aes(x,y,fill=log(zinc)),          # Colour <-> log(zinc)
               colour="black",                   # point outer colour
               pch=21, size=3) +                 # size of point
    coord_fixed() +                              # fix aspect ratio
    xlab("Easting (m)") + ylab("Northing (m)")   # axes labels

g2 <- LinePlotTheme() +                          # Similar to above but with s.e.
    geom_polygon(data=BAUs_df,
                 aes(x,y,fill=sqrt(var),group=id),
                 colour="light grey") +
    scale_fill_distiller(palette="Spectral",
                         trans="reverse",
                         guide = guide_legend(title="se")) +
    coord_fixed() +
    xlab("Easting (m)") + ylab("Northing (m)")
@

<<echo=FALSE,fig.cap="Inference at the BAU level using FRK with the \\tt{meuse} dataset. (a) FRK prediction. (b) FRK prediction standard error.\\label{fig:PredictionBAU}",fig.width=9,fig.height=11,out.width="0.5\\linewidth",fig.subcap=c('',''),fig.pos="t">>=
plot(g1)
plot(g2)
@

Now, assume that we wish to predict over regions encompassing several BAUs such that the matrix $\Cmat_P$ containes multiple non-zeros per row. Then we need to set the \tt{pred\_polys} argument in the function \tt{auto\_BAUs}. First, we create this larger regionalisation as follows

<<>>=
Pred_regions <- auto_BAUs(manifold = plane(),      # model on the 2D plane
                          cellsize = c(600,600),   # choose a large grid size
                          type = "grid",           # use a grid (not hex)
                          data = meuse,            # the dataset on which to center cells
                          convex=-0.05)            # border buffer factor
@


\noindent and carry out prediction on the larger polygons:

<<>>=
Pred_regions <- SRE.predict(SRE_model = S,                # SRE model
                            use_centroid = TRUE,          # treat BAUs as points
                            pred_polys = Pred_regions)    # prediction polygons
@

\noindent The prediction and its standard error can be visualised as before. These plots are shown in Fig.~\ref{fig:PredictionPolygon}.

<<echo=FALSE,fig.cap="Prediction and prediction standard error obtained with FRK from the \\tt{meuse} dataset over arbitrary polygons. Both quantities are logs of ppm.\\label{fig:PredictionPolygon}",fig.subcap=c("",""),fig.width=9,fig.height=11,out.width="0.5\\linewidth",fig.pos="t">>=
Pred_regions_df <- SpatialPolygonsDataFrame_to_df(sp_polys = Pred_regions,
                                          vars = c("mu","var"))

g1 <- LinePlotTheme() +
    geom_polygon(data=Pred_regions_df,
                 aes(x,y,fill=mu,group=id),
                 colour="light grey") +
    scale_fill_distiller(palette="Spectral",
                         trans="reverse") +
    geom_point(data=data.frame(meuse),
               aes(x,y,fill=log(zinc)),
               colour="black",
               pch=21, size=3) +
    coord_fixed() +
    xlab("Easting (m)") + ylab("Northing (m)")

g2 <- LinePlotTheme() +
    geom_polygon(data=Pred_regions_df,
                 aes(x,y,fill=sqrt(var),group=id),
                 colour="light grey") +
    scale_fill_distiller(palette="Spectral",
                         trans="reverse",
                         guide = guide_legend(title="se")) +
    coord_fixed() +
    xlab("Easting (m)") + ylab("Northing (m)")

plot(g1)
plot(g2)
@


\subsection{The \tt{AIRS} dataset}\label{sec:AIRS}

Modelling on the sphere proceeds in a very similar fashion to the plane, except that an unprojected coordinate reference system (CRS) for the data needs to be declared on the sphere. This is implemented using a \tt{CRS} object with string \tt{"+proj=longlat +ellps=sphere"}.

\vspace{0.1in}

\noindent {\bf Step 1:} Fifteen days of \tt{AIRS} data in May 2003 are included with \tt{FRK} and these can be loaded through the \tt{data} command:

<<eval=TRUE>>=
data(AIRS_05_2003)                                          ## Load data
@

We next subset the data to include only the first three days, rename \tt{co2std} to \tt{std} (since this is what is required by \tt{FRK} to identify the standard deviation of the measurement error), and select the columns that are relevant for the study. Finally we assign the CRS object:

<<>>=
AIRS_05_2003 <-
    dplyr::filter(AIRS_05_2003,day %in% 1:3) %>%    # only first three days
    dplyr::mutate(std=co2std) %>%                          # change std to have suitable name
    dplyr::select(lon,lat,co2avgret,std)            # select columns we actually need
coordinates(AIRS_05_2003) = ~lon+lat                # change into an sp object
proj4string(AIRS_05_2003) =
            CRS("+proj=longlat +ellps=sphere")      # unprojected coordinates on sphere
@

\vspace{0.1in}

\noindent {\bf Step 2:} The next step is to create BAUs on the sphere. This is done, again, using the \tt{auto\_BAUs} function but this time with the manifold specified to be the sphere. We also specify that we wish the BAUs to form an ISEA Aperture 3 Hexagon (ISEA3H) discrete global grid (DGG) at resolution 6. Resolutions 0--6 are included with \tt{FRK}; for higher resolutions please install the package \tt{dggrids} from \tt{https://github.com/andrewzm/dggrids}. By default, this will create a hexagonal grid on the sphere. However, it is possible to have a rectangular lattice by using \tt{type = "grid"} and specifying the \tt{cellsize} as in Section \ref{sec:meuse}; see Section \ref{sec:AIRS_ST}. An example of an ISEA3H grid, at resolution 5, is shown in Fig.~\ref{fig:sphere_BAUs}.


<<eval=TRUE>>=
isea3h_sp_poldf <- auto_BAUs(manifold   = sphere(),   # model on sphere
                             isea3h_res = 6,          # isea3h resolution 6 BAUs
                             type = "hex")            # hexagonal grid
isea3h_sp_poldf$fs = 1                                # fine-scale component
@

\vspace{0.1in}

\noindent {\bf Step 3:} Now we construct the basis functions, this time of type \tt{"bisquare"} with three resolutions. We supply the data and the argument \tt{prune} so that basis functions in regions where there are no data are omitted from the final set. We also introduce a new argument, \tt{subsamp}. This argument dictates how many data points (chosen at random) should be used when carrying out the pruning. In general, the higher \tt{nres}, the higher \tt{subsamp} should be in order to ensure that high resolution basis functions are not omitted where data is actually available. The argument \tt{subsamp} need only be used when pruning with the entire dataset consumes a lot of resources.

<<eval=TRUE>>=
G <- auto_basis(m = sphere(),       # basis functions on the sphere
                data=AIRS_05_2003,  # AIRS data
                nres = 3,           # number of resolutions
                prune=15,           # prune threshold
                type = "bisquare",  # bisquare function
                subsamp = 20000)    # prune using 20000 data points (at random)
@


<<echo=FALSE,fig.cap="BAUs and basis functions used in modelling and predicting with the \\tt{AIRS} data. (a) BAUs are the ISEA3H hexagons at resolution 5. (b) Basis function centroids constructed using the function \\tt{auto\\_basis}.\\label{fig:sphere_BAUs}",fig.subcap=c("",""),fig.width=4,fig.height=4,out.width="0.5\\linewidth",fig.pos="t!",message=FALSE>>=
data("isea3h")
ggplot(subset(isea3h,res==5 & centroid==0)) +
          geom_path(aes(lon,lat,group=id)) +
          coord_map("ortho") +
          xlab("lon (deg)") +
          ylab("lat (deg)")
show_basis(G,draw_world()) +
    coord_fixed(ratio = 2) +
          xlab("lon (deg)") +
          ylab("lat (deg)")
@

\vspace{0.1in}

\noindent {\bf Steps 4--5:} Since CO$_2$ mole fraction has a latitudinal gradient, we use latitude as a covariate in our model. The SRE object is then constructed in the same way as Section \ref{sec:meuse}, but this time we set \tt{est\_error = FALSE} since the measurement error is supplied with the data. When multiple data points fall into the same BAU, we assume that each of these data points are conditionally independent readings of the process in the BAU. The matrix $\Vmat_Z$ is therefore not diagonal and this increases computational time considerably. For large point-referenced datasets, such as the \tt{AIRS} dataset, one can leave the argument \tt{average\_in\_BAU = TRUE} set (by default) to indicate that one wishes to summarise the data at the BAU level. Below, and in all subsequent analyses, we set the maximum number of EM iterations \tt{n\_EM = 2} so as to reduce the computational time of the vignette:

<<cache=FALSE,eval=TRUE>>=
f <- co2avgret ~ lat + 1         # formula for fixed effects
S <- SRE(f = f,                  # formula for fixed effects
         list(AIRS_05_2003),     # list of data objects
         basis = G,              # basis functions
         BAUs = isea3h_sp_poldf, # BAUs
         est_error = FALSE,      # do not estimate meas. error
         average_in_BAU = TRUE)  # summarise data

S <- SRE.fit(SRE_model = S,    # SRE model
             n_EM = 2,       # max. no. of EM iterations
             tol = 0.01,       # tolerance at which EM is assumed to have converged
             print_lik=FALSE)  # do not print log-likelihood at each iteration
@

\vspace{0.1in}

\noindent {\bf Step 6:} We now predict at the BAU level but this time include a flag \tt{include\_fs = FALSE}, which indicates that we wish to carry out inferences over $(\Yvec_P - \deltab_P)$ rather than over $\Yvec_P$; see \eqref{eq:smooth1} and \eqref{eq:smooth2}. This is sometimes required when one is interested in the medium-to-small scale variation; maps of the FRK prediction generated with this flag set are rather smooth (since the basis functions adopted tend to be smooth) and the prediction standard error tends to be low compared to what one would obtain when predicting $\Yvec_P$. Of course, $\var(\Yvec_P - \deltab_P \mid \Zvec) \ne \var(\Yvec_P \mid \Zvec)$.

<<eval=TRUE>>=
isea3h_sp_poldf <- SRE.predict(SRE_model = S,          # SRE model
                               use_centroid = TRUE,    # use centroid as point reference
                               include_fs = FALSE)     # do not predict fs-variation
@

The prediction and prediction standard error maps, together with the observation data, are shown in Figs.~\ref{fig:AIRSresults1}--\ref{fig:AIRSresults3}.

<<echo=FALSE,results='hide',message=FALSE>>=
X <- SpatialPolygonsDataFrame_to_df(sp_polys = isea3h_sp_poldf,
                                    vars = c("mu","var"))

mumin <- min(X$mu)
mumax <- max(X$mu)
@

<<echo=FALSE,fig.width=7,fig.height=3.5,out.width="\\linewidth",fig.pos="t!",fig.cap="CO$_2$ mole-fraction readings in ppm from the \\tt{AIRS}.\\label{fig:AIRSresults1}",fig.align="centre">>=
g1 <- (EmptyTheme() +
           geom_point(data=data.frame(AIRS_05_2003),
                      aes(lon,lat,
                          colour=pmin(pmax(
                              co2avgret,mumin),
                              mumax)),
                      pch=46) +
           scale_colour_distiller(palette="Spectral",
                                  guide_legend(title="co2")
           ) +
           coord_map("mollweide")) %>%
    draw_world(inc_border=TRUE) +
          xlab("lon (deg)") +
          ylab("lat (deg)")
print(g1)
@

<<echo=FALSE,fig.width=7,fig.height=3.5,out.width="\\linewidth",fig.pos="t!",fig.cap="Prediction of $(\\Yvec_P - \\deltab_P)$ in ppm following FRK on the \\tt{AIRS} data.\\label{fig:AIRSresults2}",fig.align="centre">>=
g2 <- (EmptyTheme() +
           geom_polygon(data=X,
                        aes(lon,lat,fill=mu,group=id))+
           scale_fill_distiller(palette="Spectral") +
           coord_map("mollweide")) %>%
    draw_world(inc_border=TRUE)+
          xlab("lon (deg)") +
          ylab("lat (deg)")
print(g2)
@

<<echo=FALSE,fig.keep=TRUE,fig.width=7,fig.height=3.5,out.width="\\linewidth",fig.pos="t!",fig.cap="Prediction standard error of $(\\Yvec_P - \\deltab_P)$ in ppm following FRK on the \\tt{AIRS} data.\\label{fig:AIRSresults3}",fig.align="centre">>=
X$se <- pmin(sqrt(X$var),0.5)
g3 <- (EmptyTheme() +
           geom_polygon(data=X,
                        aes(lon,lat,fill=se,group=id))+
           scale_fill_distiller(palette="Spectral") +
           coord_map("mollweide")) %>%
    draw_world(inc_border=TRUE)+
          xlab("lon (deg)") +
          ylab("lat (deg)")

print(g3)
@

\section{Fixed Rank Kriging in space and time}\label{sec:spacetime}

Although \tt{FRK} is primarily designed for spatial data, it also has functionality for modelling and predicting with spatio-temporal data. The functionality in the software is limited in some respects; in particular, only point-referenced spatio-temporal data can be used (that will be subsequently mapped to BAUs), and estimation of the standard deviation of the measurement error is not implemented. These features will be implemented in future revisions.

Fixed Rank Kriging is space and time is different from Fixed Rank Filtering \citep{Cressie_2010} where a temporal auto-regressive structure is imposed on the basis-function weights $\{\etab_t \}$, and where subsequently Kalman filtering and Rauch-Tung-Striebel smoothing are used for inference on $\{\etab_t \}$. In FRK, the basis functions also have a temporal dimension; the only new aspect is specifying these space-time basis functions.

We illustrate FRK in space and time using two datasets. The first dataset we consider was obtained from the National Oceanic and Atmospheric Administration (NOAA), and we will term it the \tt{NOAA} dataset. This dataset is included in \tt{FRK}, and it contains daily observations of maximum temperature (\tt{Tmax}) in degrees Fahrenheit at 138 stations in the US between between 32N--46N and 80W--100W, recorded between the years 1990 and 1993 (inclusive); see Fig.~\ref{fig:stat_locs}. We will only consider the 31 maximum temperatures recorded in July 1993 in this vignette. The second dataset we use is the same \tt{AIRS} dataset referred to in Section \ref{sec:AIRS}. In Section \ref{sec:AIRS} only the first 3 days were used to illustrate spatial-only FRK; we now use all 15 days to illustrate spatio-temporal FRK.

For creating spatio-temporal objects used by \tt{FRK} we  need to load the \tt{spacetime} package:

<<>>=
library(spacetime)
@

\subsection{The \tt{NOAA} dataset} \label{sec:NOAA}

\noindent {\bf Step 1:} We load the dataset and extract the data for July 1993 using the commands

<<message=FALSE>>=
data("NOAA_df_1990")             # load data
Tmax <- subset(NOAA_df_1990,     # subset the data
              month %in% 7 &     # May to July
              year == 1993)      # year of 1993
@

<<echo=FALSE,fig.height=4,fig.width=6,fig.cap="Station locations from which the maximum temperature readings in the \\tt{NOAA} dataset were obtained.\\label{fig:stat_locs}",out.width="0.6\\linewidth">>=
(LinePlotTheme() + geom_point(data=Tmax,aes(lon,lat),size=0.5)) %>%
    draw_world() + coord_fixed(xlim = c(-115,-60), ylim = c(10,60))


@

\noindent To construct a spatio-temporal object, one must first define the temporal component as a \tt{Date} object by stringing the year, month and day together:
<<>>=
Tmax <- within(Tmax,
               {time = as.Date(paste(year,month,day,sep="-"))})  # create Date field
@
\noindent Since the data is point-referenced, we need to cast our data into a `spatio-temporal irregular data frame', \tt{STIDF}; refer to the vignette \tt{JSS816} for various ways to do this. One of the most straightforward approaches is to use the function \tt{stConstruct} in the package \tt{spacetime}. The function needs to be supplied along with the data, the names of the spatial coordinates field, the name of the Date field, and a flag indicating whether the data can be treated as having been recorded over the temporal interval and not at the specific instant recorded in \tt{time} (in our case \tt{interval=TRUE}).

<<>>=
STObj <- stConstruct(x = Tmax,                    # dataset
                 space = c("lon","lat"),          # spatial fields
                 time="time",                     # time field
                 interval=TRUE)                   # time reflects an interval
@

\noindent Unlike for the spatial-only case, the standard deviation of the measurement error needs to be specified. In this case, we conservatively set it to be 2 degrees Fahrenheit, although it is likely to be much less in practice. We also treat the data as being on $\mathbb{R}^2$ (that is, where space is the plane; we consider space-time data where space is the surface of a sphere in Section \ref{sec:AIRS_ST}):

<<>>=
STObj$std <- 2
@

\vspace{0.1in}

\noindent {\bf Step 2:} When dealing with spatio-temporal data, the BAUs are space-time regular lattices, which are classified in \tt{spacetime} as a `spatio-temporal fixed data frame', \tt{STFDF}; type \tt{help(STFDF)} for details.  \tt{STFDF} objects may be constructed manually or by using the helper function \tt{auto\_BAUs}. In the following, the helper function is used to construct BAUs in a space-time cube, centred around the data \tt{STObj}, with each BAU of size 1 deg. latitude $\times$ 1 deg. longitude $\times$ 1 day. The new arguments here are \tt{manifold = STplane()}, which indicates that we are going to model a spatio-temporal field on the 2D plane, and \tt{tunit = "days"}, which indicates that each BAU has a temporal `width' equal to one day. Once again, we specify the fine-scale component to be homoscedastic:

<<warning=FALSE>>=
grid_BAUs <- auto_BAUs(manifold=STplane(),    # spatio-temporal process on the plane
                       data=STObj,            # data
                       cellsize = c(1,1,1),   # BAU cell size
                       type="grid",           # grid or hex?
                       convex=-0.1,           # parameter for hull construction
                       tunit="days")          # time unit
grid_BAUs$fs = 1                       # fine-scale variation
@

\vspace{0.1in}

\noindent {\bf Step 3:} The simplest way to construct spatio-temporal basis functions is to first construct spatial basis functions, then temporal basis functions, and then combine them by taking their tensor product. To construct spatial basis functions, we first project the spatio-temporal data onto the spatial domain (collapse out time) using \tt{as(STObj,"Spatial")}, and then construct spatial basis function using \tt{auto\_basis}:

<<>>=
G_spatial <- auto_basis(m = plane(),                 # spatial functions on the plane
                        data=as(STObj,"Spatial"),    # remove the temporal dimension
                        nres = 3,                    # three resolutions
                        type = "bisquare")           # bisquare basis functions
@

\noindent For the temporal basis functions, we use the function \tt{local\_basis}, which gives the user more control over the location parameters and the scale parameters of the basis functions. In this case we specify that we want basis functions on the real line, located between $t = 2$ and $t = 28$ at an interval spacing of 4. Here, each location represents a temporal interval used in the construction of \tt{grid\_BAUs}; for example, $t = 1$ corresponds to 1993-07-01, $t = 5$ to 1993-07-05, and so on.

<<warning=FALSE>>=
print(head(grid_BAUs@time))                                # show time indices
G_temporal <- local_basis(manifold = real_line(),          # functions on the real line
                           type = "Gaussian",              # Gaussian functions
                           loc = matrix(seq(2,28,by=4)),   # locations of functions
                           scale = rep(3,7))               # scales of functions
@

\noindent The basis functions can be visualised using \tt{show\_basis}. The generated basis functions are shown in Figure \ref{fig:STbasis}:

<<message=FALSE>>=
basis_s_plot <- show_basis(G_spatial) + xlab("lon (deg)") + ylab("lat (deg)")
basis_t_plot <- show_basis(G_temporal) + xlab("time index") + ylab(expression(phi(t)))
@


<<echo=FALSE,fig.height=4,fig.width=4,fig.subcap=c("",""),fig.cap="Spatial and temporal basis functions used to construct the spatio-temporal basis functions. (a)  Spatial support of the bisquare spatial basis functions. (b) The temporal basis functions.\\label{fig:STbasis}",out.width="0.5\\linewidth">>=
print(basis_s_plot)
print(basis_t_plot)
@

\noindent The spatio-temporal basis functions are then constructed using the function \tt{TensorP} as follows:

<<>>=
G <- TensorP(G_spatial,G_temporal)         # take the tensor product
@

\vspace{0.1in}

\noindent {\bf Steps 4--6:} We next construct the SRE model, using an intercept and latitude as fixed effects, \tt{STObj} as the data, \tt{G} as the set of basis functions, and \tt{grid\_BAUs} as the BAUs. We also specify \tt{est\_error = FALSE} since this functionality is currently not implemented for spatio-temporal data. The SRE model is then fitted using the familiar command \tt{SRE.fit} and prediction is carried out using \tt{SRE.predict}:

<<SRE,results='hide',cache=FALSE>>=
 f <- z ~ 1 + lat                 # fixed effects part
 S <- SRE(f = f,                  # formula
          data = list(STObj),     # data (can have a list of data)
          basis = G,              # basis functions
          BAUs = grid_BAUs,       # BAUs
          est_error = FALSE)      # do not estimate measurement-error variance

 S <- SRE.fit(SRE_model = S,    # estimate parameters in the SRE model S
             n_EM = 2,        # maximum no. of EM iterations
             tol = 0.1,         # tolerance on log-likelihood
             print_lik=FALSE)   # print log-likelihood trace

 grid_BAUs <- SRE.predict(SRE_model = S,         # SRE model
                          use_centroid = TRUE)   # Treat BAUs as points
@

Plotting proceeds precisely the same way as in Section \ref{sec:meuse}, however now we need to convert the spatial polygons at multiple time points to data frames. This can be done by simply iterating through the time points we wish to visualise. In the following, we extract a data frame for the BAUs on days 1, 4, 8, 12, 16, and 20. The prediction and the prediction standard error are shown in Figs.~\ref{fig:FRK_pred1} and \ref{fig:FRK_pred2}, respectively. Note how the prediction has both smooth and fine-scale components. This is expected, since fine-scale variation was, this time, included in the prediction. Note also that the BAUs we used are not located everywhere within the square domain of interest. This is because the \tt{auto\_BAUs} function carefully chooses a (non-square) domain in an attempt to minimise the number of BAUs needed. This can be adjusted by changing the \tt{convex} parameter in \tt{auto\_BAUs}.

<<message=FALSE,results='hide'>>=
 analyse_days <- c(1,4,8,12,16,20)  # analyse only a few days
 df_st <- lapply(analyse_days,      # for each day
        function(i)
            SpatialPolygonsDataFrame_to_df(grid_BAUs[,i],  # convert to df
                                           c("mu","var")) %>%
            cbind(day = i))         # add day number to df
 df_st <- do.call("rbind",df_st)    # append all dfs together
@


<<echo=FALSE,fig.cap="Spatio-temporal FRK prediction of \\tt{Tmax} on the plane in degrees Fahrenheit within a domain enclosing the region of interest for six selected days spanning the temporal window of the data: 01 July 1993 -- 20 July 2003.\\label{fig:FRK_pred1}",fig.height=8,fig.width=16,fig.pos="t!">>=
LinePlotTheme() +                          # Similar to above but with s.e.
    geom_polygon(data=df_st,
                 aes(lon,lat,fill=mu,group=id),
                 colour="light grey") +
      geom_point(data=filter(Tmax,day %in% c(1,4,8,12,16,20)),           # Plot data
               aes(lon,lat,fill=z),          # Colour <-> log(zinc)
               colour="black",                   # point outer colour
               pch=21, size=3) +                 # size of point
    scale_fill_distiller(palette="Spectral",
                         guide = guide_legend(title="se")) +
    coord_fixed() +
    xlab("lon (deg)") + ylab("lat (deg)") +
    facet_wrap(~day)
@

<<echo=FALSE,fig.cap="Spatio-temporal FRK prediction standard  error of  \\tt{Tmax} on the plane in degrees Fahrenheit within a domain enclosing the region of interest for the same six days selected in Fig.~\\ref{fig:FRK_pred1} and spanning the temporal window of the data, 01 July 1993 -- 20 July 2003.\\label{fig:FRK_pred2}",fig.height=8,fig.width=16,fig.pos="t!">>=
LinePlotTheme() +                          # Similar to above but with s.e.
    geom_polygon(data=df_st,
                 aes(lon,lat,fill=sqrt(var),group=id),
                 colour="light grey") +
      scale_fill_distiller(palette="Spectral",
                         trans="reverse",
                         guide = guide_legend(title="se")) +
    coord_fixed() +
    xlab("lon (deg)") + ylab("lat (deg)") +
    facet_wrap(~day)
@

In order to model the \tt{NOAA} dataset on a subset of the sphere, we first need to associate an appropriate Coordinate Reference System with \tt{STObj},

<<>>=
proj4string(STObj) <- "+proj=longlat +ellps=sphere"
@

\noindent  and then adjust the BAUs and basis functions used. This entails using \tt{STsphere()} instead of \tt{STplane()} in BAU construcation and \tt{sphere()} instead of \tt{plane()} in spatial-basis-function construction:

<<FRK2,cache=FALSE>>=
grid_BAUs <- auto_BAUs(manifold=STsphere(),       # spatio-temporal process on the sphere
                       data=STObj,                # data
                       cellsize = c(1,1,1),       # BAU cell size
                       type="grid",               # grid or hex?
                       convex=-0.1,               # parameter for hull construction
                       tunit="days")              # time unit

 G_spatial <- auto_basis(m = sphere(),              # spatial functions on the plane
                        data=as(STObj,"Spatial"),  # remove the temporal dimension
                        nres = 6,                  # six resolutions of DGG
                        type = "bisquare",         # bisquare basis functions
                        prune=15,                  # prune basis functions
                        isea3h_lo = 4)             # but remove those lower than res 4
@

Recall that when calling \tt{auto\_basis} on the sphere, basis functions are automatically constructed at locations specified by the DGGs. In the code given above, we use the first six resolutions (resolutions 0 -- 5) of the DGGs but discard resolutions less than 4 by using the argument \tt{isea3h\_lo = 4}. The \tt{prune=1} argument behaves as above on the basis functions at these higher resolutions. The basis functions constructed using this code are shown in Fig.~\ref{fig:basis_USA}. We provide more details on FRK on the sphere in Section \ref{sec:AIRS_ST}.

<<message=FALSE,echo=FALSE,fig.cap="Basis functions for FRK on the sphere with the \\tt{NOAA} dataset using two ISEA3H DGGs for location parameters of the basis functions.\\label{fig:basis_USA}",fig.height=9,fig.width=9,fig.pos="t!",out.width="0.7\\linewidth",fig.align="center">>=
draw_world(show_basis(G_spatial,LinePlotTheme())) + coord_map("ortho",orientation = c(35,-100,0)) + xlab("lon (deg)") + ylab("lat (deg)")
@

\subsection{The \tt{AIRS} dataset} \label{sec:AIRS_ST}

In this section we use spatio-temporal FRK on the sphere to obtain FRK predictions and prediction standard errors of CO$_2$, in ppm, between 01 May 2003 and 15 May 2003 (inclusive). To keep the package size small, the dataset \tt{AIRS\_05\_2003} only contains data in these first 15 days of May 2003.

\vspace{0.1in}

\noindent {\bf Step 1:} First we load the dataset that is available with \tt{FRK}:

<<eval=TRUE>>=
data(AIRS_05_2003)   # load AIRS data
@
\noindent and then rename \tt{co2std} to \tt{std} and attribute the time index \tt{t} to the day number. We also use 20000 data points chosen at random between 01 May 2003 and 15 May 2003 (inclusive) in order to keep the compilation time of the vignette low.

<<eval=TRUE>>=
set.seed(1)
AIRS_05_2003 <- mutate(AIRS_05_2003,           # take the data
                       std=co2std,             # rename std
                       t = day)   %>%          # generate time index
                sample_n(20000)                # sample 20000 points
@

As with the \tt{NOAA} dataset, we create a date field using \tt{as.Date}
<<>>=
AIRS_05_2003 <- within(AIRS_05_2003,
               {time = as.Date(paste(year,month,day,sep="-"))})  # create Date field
@

\noindent and construct the spatio-temporal object (\tt{STIDF}) using \tt{stConstruct}:

<<>>=
STObj <- stConstruct(x = AIRS_05_2003,            # dataset
                 space = c("lon","lat"),          # spatial fields
                 time ="time",                    # time field
                 crs = CRS("+proj=longlat +ellps=sphere"),  # CRS
                 interval=TRUE)                   # time reflects an interval
@


%' <<>>=
%' time <- as.POSIXct("2003-05-01",tz="") + 3600*24*(AIRS_05_2003$t-1)
%' space <- AIRS_05_2003[,c("lon","lat")]
%' coordinates(space) = ~lon+lat # change into an sp object
%' proj4string(space)=CRS("+proj=longlat +ellps=sphere")
%' STObj <- STIDF(space,time,data=AIRS_05_2003)
%' # stplot(STObj[,,"co2avgret"])
%' @

\vspace{0.1in}

\noindent {\bf Step 2:} We next construct the BAUs. This time we specify \tt{STsphere()} for the manifold, and for illustration we discretise the sphere using a regular grid rather than a hexagonal lattice.  To do this we set a \tt{cellsize} and specify \tt{type="grid"}. We also supply \tt{time(STObj)} so that the BAUs are constructed around the time of the data; if we supply \tt{STObj} instead, then BAUs are pruned spatially.  We show the BAUs generated using \tt{time(STObj)} and \tt{STObj} in Fig.~\ref{fig:sphere_grid_BAUs}.


<<eval=TRUE,cache=FALSE>>=
## Prediction (BAU) grid
grid_BAUs <- auto_BAUs(manifold=STsphere(),   # space-time field on sphere
                             data=time(STObj),      # temporal part of the data
                             cellsize = c(5,5,1),   # cellsize (5 deg x 5 deg x 1 day)
                             type="grid",           # grid (not hex)
                             tunit = "days")        # time spacing in days
grid_BAUs$fs = 1
@



<<echo=FALSE,fig.subcap=c("",""),fig.cap="Gridded BAUs on the sphere used for modelling and predicting with the \\tt{AIRS} data. (a) BAUs constructed when supplying only the temporal indices of the data (the entire sphere is covered with BAUs within the specified time period). (b) BAUs constructed when supplying the entire dataset. The view of the sphere is from the bottom; in this case there is no data below $60^{\\circ}$S and thus BAUs have been omitted from this region.\\label{fig:sphere_grid_BAUs}",fig.width=4,fig.height=4,out.width="0.5\\linewidth",fig.pos="t!",message=FALSE>>=

grid_BAUs2 <- auto_BAUs(manifold=STsphere(),  # space-time field on sphere
                             data=STObj,            # data
                             cellsize = c(5,5,1),   # cellsize (5 deg x 5 deg x 1 day)
                             type="grid",           # grid (not hex)
                             tunit = "days")        # time spacing in days

X <- SpatialPolygonsDataFrame_to_df(grid_BAUs[,1],"n")
X2 <- SpatialPolygonsDataFrame_to_df(grid_BAUs2[,1],"n")
ggplot(X) +
    geom_polygon(aes(lon,lat,group=id),colour="black",fill="white") +
    coord_map("ortho",orientation = c(-145,125,25)) +
    xlab("lon (deg)") + ylab("lat (deg)")
ggplot(X2) +
    geom_polygon(aes(lon,lat,group=id),colour="black",fill="white") +
    coord_map("ortho",orientation = c(-145,125,25)) +
    xlab("lon (deg)") + ylab("lat (deg)")
@

\vspace{0.1in}

\noindent {\bf Step 3:} We next construct the spatio-temporal basis functions. This proceeds in exactly the same way as in the \tt{NOAA} dataset: We first construct spatial basis functions, then temporal basis functions, and then we find their tensor product:

<<eval=TRUE>>=
G_spatial <- auto_basis(m = sphere(),             # functions on sphere
                        data=as(STObj,"Spatial"), # collapse time out
                        nres = 3,                 # use three DGGRID resolutions
                        prune=15,                 # prune basis functions
                        type = "bisquare",        # bisquare basis functions
                        subsamp = 20000)          # use only 20000 data points for pruning

G_temporal <- local_basis(manifold=real_line(),      # functions on real line
                          loc = matrix(c(2,7,12)),   # location parameter
                          scale = rep(3,3),          # scale parameter
                          type = "Gaussian")
G_spacetime <- TensorP(G_spatial,G_temporal)
@

\vspace{0.1in}

\noindent {\bf Steps 4--6:} Finally, the SRE model is constructed and fitted as in the other examples. Recall that \tt{est\_error = FALSE} is required with spatio-temporal data and, since we have multiple data per BAU we also set \tt{average\_in\_BAU = TRUE}. For predicting, we use the \tt{pred\_time} flag to indicate at which time points we wish to predict; here the numbers correspond to the time indices of the BAUs. We specify \tt{pred\_time = c(4,8,12)}, which indicates that we want to predict on the 4, 8 and 12 May 2003. The data, prediction, and prediction standard error for these days are given in Figs.~\ref{fig:FRK_AIRS_ST1}--\ref{fig:FRK_AIRS_ST3}.

<<eval=TRUE,message=FALSE,results='hide',cache=FALSE>>=
f <- co2avgret ~ lat +1           # formula for fixed effects
S <- SRE(f = f,                   # formula
         data = list(STObj),      # spatio-temporal object
         basis = G_spacetime,     # space-time basis functions
         BAUs = grid_BAUs,        # space-time BAUs
         est_error = FALSE,       # do not estimate measurement error
         average_in_BAU = TRUE)   # average data that fall inside BAUs

S <- SRE.fit(SRE_model = S,       # SRE model
             n_EM = 2,          # max. EM iterations
             tol = 0.01)          # convergence criteria

grid_BAUs <- SRE.predict(SRE_model = S,          # SRE model
                         use_centroid = TRUE,    # Assume BAUs are points
                         include_fs = FALSE,     # do not include fs variation in pred.
                         pred_time = c(4,8,12))  # predict only at select days
@

<<echo=FALSE,message=FALSE,results='hide'>>=
X <- lapply(1:length(time(grid_BAUs)),
            function(i) {
                SpatialPolygonsDataFrame_to_df(sp_polys = grid_BAUs[,i],
                                    vars = c("mu","var")) %>%
            mutate(t = as.vector(grid_BAUs@time[i]))})
X <- do.call("rbind",X)
mumin <- min(X$mu)
mumax <- max(X$mu)
@




<<echo=FALSE, fig.keep=TRUE,fig.cap="CO$_2$ readings taken from the \\tt{AIRS} on the 04, 08 and 12 May 2003 in ppm. \\label{fig:FRK_AIRS_ST1}",fig.align="center",out.width="\\linewidth",fig.height=3,fig.width=16,fig.pos="t!">>=
g1 <- (EmptyTheme() +
           geom_point(data=dplyr::filter(AIRS_05_2003,t %in% c(4,8,12)),
                      aes(lon,lat,
                          colour=pmin(pmax(
                              co2avgret,mumin),
                              mumax)),
                      pch=46) +
           facet_grid(~t)+
           scale_colour_distiller(palette="Spectral",
                                  guide_legend(title="co2")) +
           coord_map("mollweide") +
            xlab("lon (deg)") +
            ylab("lat (deg)")) %>%
    draw_world(inc_border=TRUE)
print(g1)
@

<<echo=FALSE, fig.keep=TRUE,fig.cap="Prediction of $(\\Yvec_P - \\deltab_P)$ in ppm on 04, 08, and 12 May 2003 obtained with \\tt{FRK} on the \\tt{AIRS} data. \\label{fig:FRK_AIRS_ST2}",fig.align="center",out.width="\\linewidth",fig.height=3,fig.width=16,fig.pos="t!">>=

g2 <-  (EmptyTheme() +
            geom_polygon(data=filter(X,(t %in% c(4,8,12)) & abs(lon) < 175),
                         aes(lon,lat,fill=mu,group=id))+
           scale_fill_distiller(palette="Spectral") +
           coord_map("mollweide") +
           facet_grid(~t) +
            xlab("lon (deg)") +
            ylab("lat (deg)")) %>%
    draw_world(inc_border=FALSE)

print(g2)
@

<<echo=FALSE, fig.keep=TRUE,fig.cap="Prediction standard error of $(\\Yvec_P - \\deltab_P)$ in ppm on 04, 08 and 12 May 2003 obtained with \\tt{FRK} on the \\tt{AIRS} data. \\label{fig:FRK_AIRS_ST3}",fig.align="center",out.width="\\linewidth",fig.height=3,fig.width=16,fig.pos="t!">>=

X$se <- pmin(sqrt(X$var),0.7)
g3 <-  (EmptyTheme() +
            geom_polygon(data=filter(X,(t %in% c(4,8,12)) & abs(lon) < 175),
                         aes(lon,lat,fill=se,group=id))+
           scale_fill_distiller(palette="Spectral") +
           coord_map("mollweide") +
           facet_grid(~t) +
            xlab("lon (deg)") +
            ylab("lat (deg)")) %>%
    draw_world(inc_border=FALSE)

print(g3)
@




\section{Other topics}

Sections 1--4 have introduced the core functionality of \tt{FRK}. The purpose of this section is to present additional functionality that may be of use to the analyst.


\subsection{Multiple observations with different supports}

The main advantage of using BAUs is that one can make use of multiple datasets with different spatial supports without any added difficulty. Consider the \tt{meuse} dataset. We synthesise observations with a large support by changing the \tt{meuse} object into a \tt{SpatialPolygonsDataFrame}, where each polygon is a square of size 300 m $\times$ 300 m centred around the original \tt{meuse} data point, and with the original value of zinc concentration assigned to it. Once this object is set up, which we name \tt{meuse\_pols}, the analysis proceeds in precisely the same way as in Section \ref{sec:meuse}, but with \tt{meuse\_pols} used instead of \tt{meuse}.

The prediction and the prediction standard error using \tt{meuse\_pols} are shown in Fig.~\ref{fig:meuse_large}. In Fig.~\ref{fig:meuse_large} (b) we also overlay the footprints of the observations. Note how the observations affect the prediction standard error in multiple BAUs and how regions with multiple overlapping observations have lower prediction error than those with no overlap. As expected, the prediction standard error is, overall, considerably higher than that in Fig.~\ref{fig:PredictionBAU}. Note also that the supports of the observations and the BAUs do not precisely overlap. For simplicity, we assumed that an observation influences a BAU only if the centroid of the BAU lies within the observation footprint. Refining this will require a more detailed consideration of the BAU and observation footprint geometry and will be considered in future revision.

<<echo=FALSE,include=FALSE,warning=FALSE>>=
# Generate observations with large spatial support
data(meuse.grid)
data(meuse)

meuse_pols <- NULL
offset <- 150
for(i in 1:nrow(meuse)) {
    this_meuse <- meuse[i,]
    meuse_pols <- rbind(meuse_pols,
                        data.frame(x = c(this_meuse$x - offset,
                                         this_meuse$x + offset,
                                         this_meuse$x + offset,
                                         this_meuse$x - offset),
                                   y = c(this_meuse$y - offset,
                                         this_meuse$y - offset,
                                         this_meuse$y + offset,
                                         this_meuse$y + offset),
                                   id = i,
                                   zinc = this_meuse$zinc))
}
meuse_pols <- df_to_SpatialPolygons(meuse_pols,coords=c("x","y"),keys="id",proj = CRS())
meuse_pols <- SpatialPolygonsDataFrame(meuse_pols,data.frame(row.names = row.names(meuse_pols),zinc=meuse$zinc))
coordnames(meuse_pols) <- c("x","y")
coordinates(meuse) = ~x + y
@


<<message=FALSE,echo=FALSE,cache=FALSE,results='hide'>>=
set.seed(1)
GridBAUs2 <- auto_BAUs(manifold = plane(),     # 2D plane
                     cellsize = c(100,100),   # BAU cellsize
                     type = "grid",           # grid (not hex)
                     data = meuse,            # data around which to create BAUs
                     convex=-0.05)            # border buffer factor
GridBAUs2$fs <- 1   # fine-scale variation at BAU level
G <- auto_basis(m = plane(),          # 2D plane
                data=meuse,           # meuse data
                nres = 2,             # number of resolutions
                prune=5,              # prune threshold
                type = "Gaussian",    # type of basis function
                regular = 0)          # place irregularly in domain
f <- log(zinc) ~ 1    # formula for SRE model
S <- SRE(f = f,                # formula
         data = list(meuse_pols),   # list of datasets
         BAUs = GridBAUs2,      # BAUs
         basis = G,            # basis functions
         est_error=TRUE)       # estimation measurement error
S <- SRE.fit(SRE_model = S,    # SRE model
             n_EM = 2,       # max. no. of EM iterations
             tol = 0.01,       # tolerance at which EM is assumed to have converged
             print_lik=FALSE)   # print log-likelihood at each iteration
GridBAUs2 <- SRE.predict(SRE_model = S,          # SRE model
                        use_centroid = TRUE)    # use centroid as point reference
BAUs_df <- SpatialPolygonsDataFrame_to_df(sp_polys = GridBAUs2,   # BAUs to convert
                                          vars = c("mu","var"))  # fields to extract
Obs_df <- SpatialPolygonsDataFrame_to_df(sp_polys = meuse_pols,   # BAUs to convert
                                          vars = c("zinc"))  # fields to extract
g1 <- LinePlotTheme() +                          # Use a plain theme
    geom_polygon(data=BAUs_df ,                  # Draw BAUs
                 aes(x,y,fill=mu,group=id),      # Colour <-> Mean
                 colour="light grey") +          # Border is light grey
    scale_fill_distiller(palette="Spectral")  +  # Spectral palette
    coord_fixed() +                              # fix aspect ratio
    xlab("Easting (m)") + ylab("Northing (m)")   # axes labels

g2 <- LinePlotTheme() +                          # Similar to above but with s.e.
    geom_polygon(data=BAUs_df,
                 aes(x,y,fill=sqrt(var),group=id),
                 colour="light grey") +
    scale_fill_distiller(palette="Spectral",
                         trans="reverse",
                         guide = guide_legend(title="se")) +
    coord_fixed() +
    xlab("Easting (m)") + ylab("Northing (m)") +
    geom_path(data=Obs_df,
                 aes(x,y,group=id),
                 colour="black",
                 alpha = 0.5)

@


<<echo=FALSE,fig.cap="Prediction and prediction standard error obtained with FRK using the meuse dataset synthesised to have a large spatial footprint. (a) FRK prediction at the BAU level. (b) FRK prediction standard error at the BAU level. The black hexagons outline the spatial footprints of the synthesised dataset.\\label{fig:meuse_large}",fig.width=9,fig.height=11,out.width="0.5\\linewidth",fig.subcap=c("",""),fig.pos="t">>=
plot(g1)
plot(g2)
@

 \subsection{Anisotropy: Changing the distance measure}

 So far we have only considered isotropic fields. Anisotropy can be easily introduced by changing the distance measure associated with the manifold. To illustrate this, below we simulate a highly anisotropic, noisy, spatio-temporal process on a fine grid in $D = [0,1] \times [0,1]$ and sample 1000 points chosen at random from it. The process and the sampled data are shown in Fig.~\ref{fig:aniso1}.

 <<>>=
 set.seed(1)
 N <- 50
 sim_process <- expand.grid(x = seq(0.005,0.995,by=0.01),       # x grid
                            y = seq(0.001,0.995,by=0.01)) %>%   # y grid
     mutate(proc = cos(x*40)*cos(y*3) + 0.3*rnorm(length(x)))   # anisotropic function

 sim_data <- sample_n(sim_process,1000) %>%                     # sample data from field
     mutate(z = proc + 0.1*rnorm(length(x)),                    # add noise
            std = 0.1,                                          # with 0.1 std
            x = x + runif(1000)*0.001,                          # jitter x locations
            y = y + runif(1000)*0.001)                          # jitter y locations
 coordinates(sim_data) = ~x + y                                 # change into SpatialPoints
@

 <<echo=FALSE,eval=TRUE,fig.cap="FRK with anisotropic fields. (a) Simulated process. (b) Observed data. \\label{fig:aniso1}",fig.width=6,fig.height=5,out.width="0.5\\linewidth",fig.subcap=c('',''),fig.pos="t">>=
  g1 <-LinePlotTheme() +
      scale_fill_distiller(palette="Spectral") +
      geom_tile(data=sim_process,aes(x,y,fill=proc))+
      coord_fixed(xlim=c(0,1),ylim=c(0,1))

 g2 <- LinePlotTheme() +
     scale_fill_distiller(palette="Spectral") +
     geom_point(data=data.frame(sim_data),
                aes(x,y,fill=z),
                colour="black",
                pch=21, size=2) +
     coord_fixed(xlim=c(0,1),ylim=c(0,1))

   print(g1)
   print(g2)
 @

To create the modified distance measure, we note that the spatial frequency in $x$ is approximately four times that in $y$. Therefore, in order to generate anisotropy, we use a measure that scales $x$ by 4. In \tt{FRK}, a \tt{measure} object requires a distance function, and the dimension of the manifold on which it is used, as follows:
<<eval=TRUE>>=
scaler <- diag(c(4,1))                                    # scale x by 4
asymm_measure <- new("measure",                           # new measure object
                      dist=function(x1,x2)                # new distance function
                            FRK:::rdist(x1 %*% scaler,    # scaling of first point
                                        x2 %*% scaler),   # scaling of second point
                      dim=2L)                             # in 2D
@

The distance function used on the plane can be changed by assigning the object \tt{asymm\_measure} to the manifold:

<<>>=
TwoD_manifold <- plane()                 # Create R2 plane
TwoD_manifold@measure <- asymm_measure   # Assign measure
@

We now generate a grid of basis functions (at a single resolution) manually. First, we create a $5 \times 14$ grid on $D$, which we will use as centres for the basis functions. We then call the function \tt{local\_basis} to construct bisquare basis functions centred at these locations with a range parameter of 0.4. Due to the scaling used, this implies a range of 0.1 in $x$ and a range of 0.4 in $y$. Basis function number 23 is illustrated in Fig.~\ref{fig:anisobasis}.

 <<>>=
basis_locs <- seq(0,1,length=14) %>%                 # x locations
    expand.grid(seq(0,1,length=5)) %>%               # y locations
    as.matrix()                                      # convert to matrix
G <-  local_basis(manifold = TwoD_manifold,          # 2D plane
                  loc=basis_locs,                    # basis locations
                  scale=rep(0.4,nrow(basis_locs)),   # scale parameters
                  type="bisquare")                   # type of function
@

<<echo=FALSE,eval=TRUE,fig.cap="Basis function 23 of the 75 constructed to fit an anisotropic spatial field. Anisotropy is obtained by changing the \\tt{measure} object of the manifold on which the basis function is constructed.\\label{fig:anisobasis}",fig.width=6,fig.height=6,out.width="0.5\\linewidth",fig.pos="t",fig.align="center">>=
S <- eval_basis(G,as.matrix(sim_process[c("x","y")]))
sim_process$S <- S[,23]
LinePlotTheme() +
    geom_tile(data=sim_process,aes(x,y,fill=S)) +
    coord_fixed() + scale_fill_distiller(palette = "Spectral",guide =
                                            guide_legend(title=expression(phi[23])))
@

From here on, the analysis proceeds in exactly the same way as shown in all the other examples. The prediction and prediction standard error are shown in Fig.~\ref{fig:aniso2}.

 <<echo=FALSE,cache=FALSE,message=FALSE,results='hide'>>=
  ## Prediction (BAU) grid
  grid_BAUs <- auto_BAUs(manifold=plane(),
                         data=sim_data,
                         cellsize = c(0.02,0.02),
                         type="grid",
                         convex = -0.1)
  grid_BAUs$fs = 1

   f <- z ~ 1
  S <- SRE(f = f,
           data = list(sim_data),
           basis = G,
           BAUs = grid_BAUs,
           est_error = FALSE,
           average_in_BAU = FALSE)

   S <- SRE.fit(SRE_model = S,
               n_EM = 2,
               tol = 0.01)

   grid_BAUs <- SRE.predict(SRE_model = S,
                           use_centroid = TRUE)

    X <- SpatialPolygonsDataFrame_to_df(sp_polys = grid_BAUs,
                                      vars = c("mu","var")) %>%
      filter(x < 1.1 & x > -0.1 & y > -0.5 & y < 10.5)

  X$se <- sqrt(X$var)

g1 <-LinePlotTheme() +
     scale_fill_distiller(palette="Spectral") +
     geom_polygon(data=X,aes(x,y,fill=mu,group=id))+
     coord_fixed(xlim=c(0,1),ylim=c(0,1))

g2 <-LinePlotTheme() +
     scale_fill_distiller(palette="Spectral") +
     geom_polygon(data=X,aes(x,y,fill=se,group=id))+
     coord_fixed(xlim=c(0,1),ylim=c(0,1))

@

<<echo=FALSE,fig.subcap=c("",""),fig.cap="FRK using data generated by an anisotropic field. (a) FRK prediction. (b) FRK prediction standard error.\\label{fig:aniso2}",fig.width=6,fig.height=5,out.width="0.5\\linewidth",fig.pos="t">>=
print(g1)
print(g2)
@


\subsection{Customised basis functions and Basic Areal Units (BAUs)} \label{sec:custom_basis}

The package \tt{FRK} provides the functions \tt{auto\_BAUs} and \tt{auto\_basis} to help the user construct the BAUs and basis functions based on the supplied data. These, however, could be done manually. When doing so it is important that some rules are adhered to: The object containing the basis functions needs to be of class \tt{Basis}. This class contains 5 slots:
\begin{itemize}
\item {\bf dim}: The dimension of the manifold.
\item {\bf fn}: A list of functions. By default, distances in these functions are attributed with a manifold, but artbitrary distances can be used.
\item {\bf pars}: A list of parameters associated with each basis function. For the local basis functions used in this vignette (constructed using \tt{auto\_basis} or \tt{local\_basis}), each list item is a list with fields \tt{loc} and \tt{scale} where \tt{length(loc)} is equal to the dimension of the manifold and \tt{length(scale) = 1}.
\item {\bf df}: A data frame with number of rows equalling the number of basis functions, containing auxiliary information about the basis functions (e.g., resolution number).
\item {\bf n}: Integer equalling the number of basis functions.
\end{itemize}
There is no constructor yet for \tt{Basis}, and the \tt{R} command \tt{new} needs to use to create this object from scratch.

There are less restrictions for constructing BAUs. BAUs need to be stored as a \tt{SpatialPolygonsDataFrame} object, and the \tt{data} slot of this object must contain
\begin{itemize}
\item All covariates used in the model.
\item A field \tt{fs} denoting the fine-scale variation.
\item Fields that can be used to summarise the BAU as a point; typically the centroid of each polygon. The names of these fields need to be equal to those of the \tt{coordnames(BAUs)} (typically \tt{c("x","y")} or \tt{c("lon","lat")}).
\end{itemize}

\section{Future work}

The package \tt{FRK} is designed to address the majority of needs of the spatial and spatio-temporal analyst. There are a number of important features that remain to be implemented in future revisions. These are listed below:
\begin{itemize}
\item When dealing only with spatial data, and when the number of spatial data points is relatively low, the measurement error standard deviation can be estimated using variogram analysis. This is not very reliable, and estimation of this quantity is, ideally, included in the EM algorithm. This is indeed possible; however the measurement error standard deviation is confounded with the fine-scale variation when there is on average only 1 data point per BAU. Checks will need to be made to inform the user whether or not one should be attempting to estimate $\sigma^2_\epsilon$ in addition to $\sigma^2_\delta$.

\item Currently, \tt{FRK} is designed to work with local basis functions with a strict functional form. However, its structure can also accommodate basis functions that have no known functional form, such as empirical orthogonal functions (EOFs); future work will attempt to incorporate the use of such basis functions.

\item There is currently no component of the model which caters for sub-BAU process variation. This could be readily incorporated if the covariance function of the sub-BAU variation is known \citep{Wikle_2005}, however parameter estimation of parameters at this scale is likely to be problematic, and only possible when multiple point-referenced data appear in the individual BAUs.

\item Most work and testing in \tt{FRK} has been done on the real line, the 2D plane and the sphere surface ($\mathbb{S}^2$). Other manifolds can be implemented. Some, such as the 3D hyper-plane, are not too difficult to construct. Ultimately, it would be ideal if the user can specify his/her own manifold and functions that can compute the geodesic distances on the manifold.

\item Although designed for large data, \tt{FRK} begins to become slow when several hundreds of thousands of data points are used. The flag \tt{average\_in\_BAU} can be used to summarise the data, however if all data needs to be used, then (i) either very fine BAUs need to be used in order to ensure that not many observations fall within the same BAU, or (ii) one accepts the fact that many observations will fall into the BAUs and that observations will be correlated within the BAU. In either case, computational time will increase with the number of data points $m$, although the complexity is markedly less than $O(m^3)$.

\end{itemize}

The development page of \tt{FRK} is \tt{https://github.com/andrewzm/FRK}. Users are encourages to report any bugs or issues relating to the package on this page.

\section*{Acknowledgements}

Package devlopment was facilitated with \tt{devtools}, this vignette was created with \tt{knitr}, and package testing was carried out using \tt{testthat} and \tt{covr}. The package includes within it some date manipulation functions from \tt{Hmisc} and the functions \tt{rdist} and \tt{rdist.earth} from the package \tt{fields}. Some sparse matrix operations are facilitated using \tt{C} code from the software package \tt{SuiteSparse} \citep{SuiteSparse}.

\bibliography{FRK_bib}

\end{document}
